{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8UnAGSvL-B37",
    "outputId": "5b9e13e5-8d82-4883-96f4-afb786a9b13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#read datafolder from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsZOdwTJ-B4S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14mn-NKZ-B4d"
   },
   "outputs": [],
   "source": [
    "def getkey(x):\n",
    "    if x=='to_':\n",
    "        return 'Topics'\n",
    "    elif x=='pl_':\n",
    "        return 'Places'\n",
    "    elif x=='pe_':\n",
    "        return 'People'\n",
    "    elif x=='or_':\n",
    "        return 'Organizations'\n",
    "    elif x=='ex_':\n",
    "        return 'Exchanges'\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "#     print(i[1])\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)\n",
    "\n",
    "# print \"category_data: \", category_data\n",
    "#(news_categories.values).tolist()\n",
    "# print((category_data[0]))\n",
    "\n",
    "knn_cats={}\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        k=getkey(category_prefix)\n",
    "        knn_cats[k]=file.readlines()\n",
    "        knn_cats[k]=[s.rstrip('\\n') for s in knn_cats[k]]\n",
    "        strcat=\" \"\n",
    "        knn_cats[k]=[strcat.join(knn_cats[k])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "3ur9x7gU-B4s",
    "outputId": "122ba47b-522a-4e11-9093-b62cf018f98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DayRi-9-B5H"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ei7hKc29-B5S",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X, document_Y):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "\n",
    "\n",
    "\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      topics = newsline.topics.contents\n",
    "      places = newsline.places.contents\n",
    "      people = newsline.people.contents\n",
    "      orgs = newsline.orgs.contents\n",
    "      exchanges = newsline.exchanges.contents\n",
    "\n",
    "      for topic in topics:\n",
    "          document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "      for place in places:\n",
    "          document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "      for person in people:\n",
    "          document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "      for org in orgs:\n",
    "          document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "      for exchange in exchanges:\n",
    "          document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      document_Y[document_id] = document_categories\n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  document_Y = {}\n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X, document_Y)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X, document_Y)\n",
    "  return document_X, document_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "z-hXvcYsDqRr",
    "outputId": "50176c79-70fb-4a32-93aa-dc5fffa2f627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X, document_Y = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "WzIx7Ax5-B6R",
    "outputId": "edd77321-c6ee-4c8f-b37e-a20945202c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3csJyJTp-B6a"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Yo_3b04FMDBz",
    "outputId": "047e8cb7-fa67-40f3-e28f-5dd4cc42a8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Organizations': ['adb-africa', 'adb-asia', 'aibd', 'aid', 'anrpc', 'asean', 'atpc', 'bis', 'cipec', 'comecon', 'ec', 'eca', 'ecafe', 'ece', 'ecla', 'ecsc', 'ecwa', 'efta', 'eib', 'emcf', 'escap', 'euratom', 'fao', 'gatt', 'gcc', 'geplacea', 'iaea', 'iata', 'icco', 'ico-coffee', 'ico-islam', 'ida', 'iea', 'iisi', 'ilo', 'ilzsg', 'imco', 'imf', 'inro', 'irsg', 'isa', 'itc', 'iwc-whale', 'iwc-wheat', 'iwcc', 'iws', 'iwto', 'lafta', 'mfa', 'oapec', 'oecd', 'opec', 'un', 'unctad', 'who', 'worldbank'], 'Topics': ['acq', 'alum', 'austdlr', 'austral', 'barley', 'bfr', 'bop', 'can', 'carcass', 'castor-meal', 'castor-oil', 'castorseed', 'citruspulp', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'corn-oil', 'cornglutenfeed', 'cotton', 'cotton-meal', 'cotton-oil', 'cottonseed', 'cpi', 'cpu', 'crude', 'cruzado', 'dfl', 'dkr', 'dlr', 'dmk', 'drachma', 'earn', 'escudo', 'f-cattle', 'ffr', 'fishmeal', 'flaxseed', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-meal', 'groundnut-oil', 'heat', 'hk', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'inventories', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-meal', 'lin-oil', 'linseed', 'lit', 'livestock', 'lumber', 'lupin', 'meal-feed', 'mexpeso', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-meal', 'palm-oil', 'palmkernel', 'peseta', 'pet-chem', 'platinum', 'plywood', 'pork-belly', 'potato', 'propane', 'rand', 'rape-meal', 'rape-oil', 'rapeseed', 'red-bean', 'reserves', 'retail', 'rice', 'ringgit', 'rubber', 'rupiah', 'rye', 'saudriyal', 'sfr', 'ship', 'silk', 'silver', 'singdlr', 'skr', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'stg', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tapioca', 'tea', 'tin', 'trade', 'tung', 'tung-oil', 'veg-oil', 'wheat', 'wool', 'wpi', 'yen', 'zinc'], 'Exchanges': ['amex', 'ase', 'asx', 'biffex', 'bse', 'cboe', 'cbt', 'cme', 'comex', 'cse', 'fox', 'fse', 'hkse', 'ipe', 'jse', 'klce', 'klse', 'liffe', 'lme', 'lse', 'mase', 'mise', 'mnse', 'mose', 'nasdaq', 'nyce', 'nycsce', 'nymex', 'nyse', 'ose', 'pse', 'set', 'simex', 'sse', 'stse', 'tose', 'tse', 'wce', 'zse'], 'Places': ['afghanistan', 'albania', 'algeria', 'american-samoa', 'andorra', 'angola', 'anguilla', 'antigua', 'argentina', 'aruba', 'australia', 'austria', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belgium', 'belize', 'benin', 'bermuda', 'bhutan', 'bolivia', 'botswana', 'brazil', 'british-virgin-islands', 'brunei', 'bulgaria', 'burkina-faso', 'burma', 'burundi', 'cameroon', 'canada', 'cape-verde', 'cayman-islands', 'central-african-republic', 'chad', 'chile', 'china', 'colombia', 'congo', 'costa-rica', 'cuba', 'cyprus', 'czechoslovakia', 'denmark', 'djibouti', 'dominica', 'dominican-republic', 'east-germany', 'ecuador', 'egypt', 'el-salvador', 'equatorial-guinea', 'ethiopia', 'fiji', 'finland', 'france', 'french-guiana', 'gabon', 'gambia', 'ghana', 'gibraltar', 'greece', 'grenada', 'guadeloupe', 'guam', 'guatemala', 'guinea', 'guinea-bissau', 'guyana', 'haiti', 'honduras', 'hong-kong', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'ivory-coast', 'jamaica', 'japan', 'jordan', 'kampuchea', 'kenya', 'kuwait', 'laos', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'luxembourg', 'macao', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'martinique', 'mauritania', 'mauritius', 'mexico', 'monaco', 'morocco', 'mozambique', 'namibia', 'nepal', 'netherlands', 'netherlands-antilles', 'new-caledonia', 'new-zealand', 'nicaragua', 'niger', 'nigeria', 'north-korea', 'norway', 'oman', 'pakistan', 'panama', 'papua-new-guinea', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'rwanda', 'saudi-arabia', 'senegal', 'seychelles', 'sierra-leone', 'singapore', 'somalia', 'south-africa', 'south-korea', 'spain', 'sri-lanka', 'sudan', 'suriname', 'swaziland', 'sweden', 'switzerland', 'syria', 'taiwan', 'tanzania', 'thailand', 'togo', 'tonga', 'trinidad-tobago', 'tunisia', 'turkey', 'uae', 'uganda', 'uk', 'uruguay', 'us-virgin-islands', 'usa', 'ussr', 'vanuatu', 'vatican', 'venezuela', 'vietnam', 'west-germany', 'western-samoa', 'yemen-arab-republic', 'yemen-demo-republic', 'yugoslavia', 'zaire', 'zambia', 'zimbabwe'], 'People': ['abdel-hadi-kandeel', 'alfonsin', 'alhaji-abdul-ahmed', 'alptemocin', 'amato', 'andersen', 'andriessen', 'aqazadeh', 'aquino', 'arafat', 'babangida', 'balladur', 'bangemann', 'barreto', 'berge', 'beteta', 'blix', 'boesky', 'bond', 'botha', 'bouey', 'braks', 'bresser-pereira', 'brodersohn', 'brundtland', 'camdessus', 'carlsson', 'caro', 'castelo-branco', 'castro', 'cavaco-silva', 'chaves', 'chen-muhua', 'chiang-ching-kuo', 'chien', 'chirac', 'ciampi', 'colombo', 'conable', 'concepcion', 'corrigan', 'cossiga', 'crow', 'dadzie', 'dauster', 'de-clercq', 'de-kock', 'de-korte', 'de-la-madrid', 'de-larosiere', 'del-mazo', 'delamuraz', 'delors', 'dementsev', 'deng-xiaoping', 'dennis', 'dhillon', 'dominguez', 'douglas', 'du-plessis', 'duisenberg', 'dunkel', 'edelman', 'enggaard', 'eser', 'evren', 'eyskens', 'feldt', 'fernandez', 'ferrari', 'finnbogadottir', 'friedman', 'fujioka', 'gaddafi', 'gandhi', 'garcia', 'gava', 'godeaux', 'gonzalez', 'gorbachev', 'goria', 'gostyev', 'graf', 'greenspan', 'gromyko', 'grosz', 'guillaume', 'halikias', 'hamad-saud-al-sayyari', 'hannibalsson', 'haughey', 'hawke', 'he-kang', 'herrington', 'hillery', 'hisham-nazer', 'hoefner', 'hoffmeyer', 'holberg', 'holkeri', 'honecker', 'hovmand', 'howard-baker', 'husak', 'icahn', 'james-baker', 'james-miller', 'jaruzelski', 'jayme', 'johnston', 'kaminsky', 'kaufman', 'keating', 'khameini', 'khomeini', 'kiechle', 'king-fahd', 'kohl', 'koivisto', 'kondo', 'koren', 'kullberg', 'lacina', 'lange', 'languetin', 'lawson', 'lee-ta-hai', 'lee-teng-hui', 'leenanon', 'leigh-pemberton', 'leitz', 'li-peng', 'li-xiannian', 'liikanen', 'lubbers', 'lukman', 'lyng', 'machinea', 'macsharry', 'malhotra', 'mancera-aguayo', 'martens', 'martin', 'masse', 'maxwell', 'maystadt', 'medgyessy', 'messner', 'mikulic', 'milliet', 'mitterrand', 'miyazawa', 'mohammad-ibrahim-jaffrey-baluch', 'mohammad-khan-junejo', 'mohammad-yasin-khan-wattoo', 'mohammed-ahmed-al-razaz', 'mohammed-ali-abal-khail', 'mohammed-salaheddin-hamid', 'morales-bermudez', 'mousavi', 'moyle', 'mubarak', 'mulroney', 'murdoch', 'mustapha', 'nakao', 'nakasone', 'nasko', 'nemeth', 'nobrega', 'o-cofaigh', 'o-kennedy', 'oeien', 'okongwu', 'ongpin', 'ortega', 'ozal', 'palsson', 'pandolfi', 'papandreou', 'parkinson', 'paye', 'perez-de-cuellar', 'petricioli', 'pickens', 'poehl', 'pottakis', 'prawiro', 'qassemi', 'rafnar', 'rafsanjani', 'reagan', 'rezende', 'riberio-cadilhe', 'rich', 'rikanovic', 'rojas', 'romero', 'roumeliotis', 'rowland', 'rubio', 'ruder', 'ruding', 'russell', 'ryzhkov', 'saberbein', 'salinas', 'samojlik', 'santer', 'saracoglu', 'sarney', 'sartzetakis', 'sathe', 'schlueter', 'sedki', 'simitis', 'simonsen', 'singhasaneh', 'siregar', 'skaanland', 'soares', 'solchaga', 'sourrouille', 'sprinkel', 'steeg', 'stich', 'stoltenberg', 'stoph', 'strougal', 'subroto', 'suharto', 'sumita', 'suominen', 'takeshita', 'tamura', 'tavares-moreia', 'thatcher', 'timar', 'tinsulanonda', 'tiwari', 'toernaes', 'toman', 'tsovolas', 'vancsa', 'venkataraman', 'vera-la-rosa', 'verity', 'villanyi', 'vlatkovic', 'volcker', 'von-weizsaecker', 'vranitzky', 'waldheim', 'wali', 'walsh', 'wang-bingqian', 'wardhana', 'wasim-aun-jaffrey', 'wilson', 'wise', 'yeutter', 'young', 'yu-kuo-hua', 'zak', 'zhao-ziyang', 'zheng-tuobin', 'zia-ul-haq']}\n"
     ]
    }
   ],
   "source": [
    "print(category_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVsLqg1aD5rw"
   },
   "outputs": [],
   "source": [
    "words_in_body={}\n",
    "\n",
    "for i in range(len(totalX)):\n",
    "    words=totalX[i].split(' ')\n",
    "    words_in_body[i]=words    \n",
    "\n",
    "one_hot_label=[]\n",
    "for key,v in words_in_body.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        if i in category_dictionary['Topics']:\n",
    "            dict_temp['Topics']+=1\n",
    "        if i in category_dictionary['Places']:\n",
    "            dict_temp['Places']+=1\n",
    "        if i in category_dictionary['People']:\n",
    "            dict_temp['People']+=1\n",
    "        if i in category_dictionary['Exchanges']:\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if i in category_dictionary['Organizations']:\n",
    "            dict_temp['Organizations']+=1\n",
    "            \n",
    "    one_hot_label.append(dict_temp)\n",
    "    \n",
    "\n",
    "one_hot_label_list = []\n",
    "for i in one_hot_label:\n",
    "    one_hot_label_list.append(list(i.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0h6rl0FEHZzy",
    "outputId": "61be6dde-4430-432c-a5b1-cc5231833524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Organizations': 1, 'Topics': 5, 'Exchanges': 0, 'Places': 2, 'People': 0}\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_label[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D6yIQt4NE5OM",
    "outputId": "61224859-c460-4e3d-ade4-fbc73126fb63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('People', 2), ('Topics', 1), ('Organizations', 0), ('Exchanges', 0), ('Places', 0)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "true_ranks=[]\n",
    "for i in one_hot_label:\n",
    "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(true_ranks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WYxMg7hD-B6k",
    "outputId": "62996da0-a7a0-410a-ad12-cda6651e0c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18583\n",
      "7605\n"
     ]
    }
   ],
   "source": [
    "def accuracy_knn(predicted,actual,length):\n",
    "    tp=0\n",
    "    for i in range(length):\n",
    "        if predicted[i][0][0]==actual[i][0][0] or predicted[i][1][0]==actual[i][1][0] or predicted[i][2][0]==actual[i][2][0] :\n",
    "            tp+=1\n",
    "    \n",
    "    return tp\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "print(len(totalX))\n",
    "# tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set) \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "resultsknn=[]\n",
    "for i in totalX[0:10000]:\n",
    "    sim={}\n",
    "    for k,j in knn_cats.items():\n",
    "        sim[k]=(cosine_sim(i,j[0]))\n",
    "    resultsknn.append(sorted(sim.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "accuracy=accuracy_knn(resultsknn,true_ranks,10000)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IrXKulSIXCIo",
    "outputId": "5f549a0c-0ae6-44be-b532-52a603ccf831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7605\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy/float(10000)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Category_ranking_knn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
