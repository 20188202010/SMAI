{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2m59m68Q1qld",
    "outputId": "a1f433e8-5833-4f66-d884-0eb5f7eca0a2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ad87900579b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read datafolder from Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/SMAI/Assignments&Projects/Project/reuters21578/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#read datafolder from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/SMAI/Assignments&Projects/Project/reuters21578/'\n",
    "\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLW4-GOK1ql1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vST3KK_1qmA"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "hrlSiXQl1qmP",
    "outputId": "0786ff77-11be-436d-a73c-42c7dc1f1266"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5TsBaSB1qmt"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McT2IFBv1qm3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      \n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  \n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X)\n",
    "  return document_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "xsBU6exN1qnI",
    "outputId": "0f885e92-50f0-48f1-e3b2-51f9a976c1ef"
   },
   "outputs": [],
   "source": [
    "document_X = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ainmgGmf1qna",
    "outputId": "6cc6110a-f26c-467e-d208-8bee11600046"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(max_vocab_size)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZPmNo0c1qnx"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM8BABxZ2p2c"
   },
   "outputs": [],
   "source": [
    "#Create one-hot encode\n",
    "def makeOneHotEncoding(totalX):\n",
    "  words_in_body={}\n",
    "\n",
    "  for i in range(len(totalX)):\n",
    "      words=totalX[i].split(' ')\n",
    "      words_in_body[i]=words    \n",
    "\n",
    "  one_hot_label=[]\n",
    "  for key,v in words_in_body.items():\n",
    "      dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "      for i in v:\n",
    "          if i in category_dictionary['Topics']:\n",
    "              dict_temp['Topics']+=1\n",
    "          if i in category_dictionary['Places']:\n",
    "              dict_temp['Places']+=1\n",
    "          if i in category_dictionary['People']:\n",
    "              dict_temp['People']+=1\n",
    "          if i in category_dictionary['Exchanges']:\n",
    "              dict_temp['Exchanges']+=1\n",
    "          if i in category_dictionary['Organizations']:\n",
    "              dict_temp['Organizations']+=1\n",
    "\n",
    "      one_hot_label.append(dict_temp)\n",
    "\n",
    "\n",
    "  one_hot_label_list = []\n",
    "  for i in one_hot_label:\n",
    "\n",
    "      one_hot_label_list.append(list(i.values()))\n",
    "  return one_hot_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcDJITG8cQ7h"
   },
   "outputs": [],
   "source": [
    "one_hot_label_list = makeOneHotEncoding(totalX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "aU1IeQDx1qoC",
    "outputId": "d865b622-1898-405f-80bc-ee9e198d2303"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "!pip install scikit-multilearn\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "X = np.array(encoded_docs)\n",
    "Y = np.array(one_hot_label_list)\n",
    "classifier = BinaryRelevance(MultinomialNB())\n",
    "_ = classifier.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atQPkjDu6Wlf"
   },
   "outputs": [],
   "source": [
    "def getRankedOutput2(predicted_y):\n",
    "  all_sorted_x = []\n",
    "  for i in predicted_y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    sorted_x = sorted(dict_temp.items(), key=lambda kv: kv[1], reverse=True) \n",
    "    all_sorted_x.append(sorted_x)\n",
    "  return all_sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxpsfKTT3SPE"
   },
   "outputs": [],
   "source": [
    "def permissible(x, y):\n",
    "  if (abs(x-y)) < 2:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def accuracy(predicted,actual):\n",
    "    tp=0\n",
    "    fp = 0\n",
    "    tp_list = []\n",
    "    length = len(actual)\n",
    "    print(\"length: \",length)\n",
    "    for one_doc_idx in range(length):\n",
    "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
    "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
    "            tp+=1\n",
    "            x = tp\n",
    "        else:\n",
    "          fp = 0\n",
    "          x = fp\n",
    "        tp_list.append(x)\n",
    "    return tp/float(length), tp_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGDpsPvsdPjJ"
   },
   "outputs": [],
   "source": [
    "val_x_doc = readFiles(test_data = True)\n",
    "totalX_test, encoded_val_x = create_x_matrix(val_x_doc)\n",
    "ground_truth_val = makeOneHotEncoding(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34miBk7U3Pzp"
   },
   "outputs": [],
   "source": [
    "X = np.array(encoded_val_x)\n",
    "Y = np.array(ground_truth_val)\n",
    "\n",
    "val_pred = classifier.predict(X)\n",
    "val_pred_arr = val_pred.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CU7LfZKY6qFV",
    "outputId": "02ab9741-0004-4389-fb61-ae87375bab3c"
   },
   "outputs": [],
   "source": [
    "validate_y_list = val_pred_arr.tolist()\n",
    "\n",
    "newv = []\n",
    "for i in range(len(validate_y_list)):\n",
    "  validate_y_list_i = list(map(int, validate_y_list[i]))\n",
    "  newv.append(validate_y_list_i)\n",
    "\n",
    "acc,tpl=accuracy(newv, ground_truth_val)\n",
    "print (\"%f\"%acc)\n",
    "pred_rank_list = getRankedOutput2(val_pred_arr)\n",
    "actual_rank_list = getRankedOutput2(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2970
    },
    "colab_type": "code",
    "id": "5iNw0iiHjHQE",
    "outputId": "d2f522ef-0666-46bc-8cad-03b2c9644427"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)\n",
    "my_df  = pd.DataFrame(columns = ['body', 'predicted ranking', 'actual ranking', '_'])\n",
    "my_df['body'] = totalX_test\n",
    "my_df['predicted ranking'] = pred_rank_list\n",
    "my_df['actual ranking'] = actual_rank_list\n",
    "my_df['_'] = tpl\n",
    "my_df = my_df.sort_values(by='_', ascending=False).reset_index(drop=True)\n",
    "my_df[['body', 'predicted ranking', 'actual ranking']]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Binary_Relevance_Naive_Bayes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
