{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_Relevance_Naive_Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "2m59m68Q1qld",
        "outputId": "48eb6ef5-dee6-4278-bb07-6966354db642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#read datafolder from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
        "\n",
        "\n",
        "sgml_number_of_files = 21\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZLW4-GOK1ql1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0vST3KK_1qmA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read all categories\n",
        "category_data = []\n",
        "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0]])\n",
        "\n",
        "# Create category dataframe\n",
        "for i in category_data:\n",
        "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
        "news_categories = pd.DataFrame(data=category_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hrlSiXQl1qmP",
        "outputId": "052c3ca3-49b8-4afa-83ac-3050267f6e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P5TsBaSB1qmt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def cleanUpSentence(r):#, stop_words = None#\n",
        "    r = r.lower().replace(\"<br />\", \" \")\n",
        "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
        "    r = BAD_SYMBOLS_RE.sub('', r)\n",
        "\n",
        "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
        "\n",
        "    words = word_tokenize(r)\n",
        "\n",
        "    for w in words:\n",
        "        w = lemmatizer.lemmatize(w)\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "McT2IFBv1qm3",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse SGML files\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "  \n",
        "def makeDict(filename, document_X):\n",
        "  with open(filename, 'rb') as file:\n",
        "\n",
        "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
        "\n",
        "    for newsline in content('reuters'):\n",
        "      document_categories = []\n",
        "\n",
        "      document_id = newsline['newid']\n",
        "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "      if document_body == 'None':\n",
        "        continue\n",
        "\n",
        "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
        "      doc_categories = unescape(doc_categories)\n",
        "\n",
        "      document_body = unescape(document_body)\n",
        "\n",
        "      document_X[document_id] = document_body\n",
        "      \n",
        "\n",
        "def readFiles(test_data = False):\n",
        "  document_X = {}\n",
        "  \n",
        "  if test_data == True:\n",
        "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
        "    filename = data_folder + file_name\n",
        "    makeDict(filename, document_X)\n",
        "  else:\n",
        "    for i in range(sgml_number_of_files):\n",
        "      if i < 10:\n",
        "        seq = '00' + str(i)\n",
        "      else:\n",
        "        seq = '0' + str(i)\n",
        "\n",
        "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "      print('Reading file: %s' % file_name)\n",
        "      filename = data_folder + file_name\n",
        "      makeDict(filename, document_X)\n",
        "  return document_X\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xsBU6exN1qnI",
        "outputId": "b54234a5-43bd-42d9-d673-9e18625028c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "document_X = readFiles()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-000.sgm\n",
            "Reading file: reut2-001.sgm\n",
            "Reading file: reut2-002.sgm\n",
            "Reading file: reut2-003.sgm\n",
            "Reading file: reut2-004.sgm\n",
            "Reading file: reut2-005.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-007.sgm\n",
            "Reading file: reut2-008.sgm\n",
            "Reading file: reut2-009.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-013.sgm\n",
            "Reading file: reut2-014.sgm\n",
            "Reading file: reut2-015.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-018.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-020.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ainmgGmf1qna",
        "outputId": "d9a67431-3fc1-4b97-b1c9-10bc48ea722d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "def create_x_matrix(document_X):\n",
        "    totalX = []\n",
        "    for i, doc in document_X.items():\n",
        "        totalX.append(cleanUpSentence(doc))\n",
        "    max_vocab_size = 200\n",
        "    input_tokenizer = Tokenizer(max_vocab_size)\n",
        "    input_tokenizer.fit_on_texts(totalX)\n",
        "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
        "    return totalX,encoded_docs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VZPmNo0c1qnx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalX,encoded_docs=create_x_matrix(document_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nM8BABxZ2p2c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create one-hot encode\n",
        "def makeOneHotEncoding(totalX):\n",
        "  words_in_body={}\n",
        "\n",
        "  for i in range(len(totalX)):\n",
        "      words=totalX[i].split(' ')\n",
        "      words_in_body[i]=words    \n",
        "\n",
        "  one_hot_label=[]\n",
        "  for key,v in words_in_body.items():\n",
        "      dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
        "      for i in v:\n",
        "          if i in category_dictionary['Topics']:\n",
        "              dict_temp['Topics']+=1\n",
        "          if i in category_dictionary['Places']:\n",
        "              dict_temp['Places']+=1\n",
        "          if i in category_dictionary['People']:\n",
        "              dict_temp['People']+=1\n",
        "          if i in category_dictionary['Exchanges']:\n",
        "              dict_temp['Exchanges']+=1\n",
        "          if i in category_dictionary['Organizations']:\n",
        "              dict_temp['Organizations']+=1\n",
        "\n",
        "      one_hot_label.append(dict_temp)\n",
        "\n",
        "\n",
        "  one_hot_label_list = []\n",
        "  for i in one_hot_label:\n",
        "\n",
        "      one_hot_label_list.append(list(i.values()))\n",
        "  return one_hot_label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcDJITG8cQ7h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_hot_label_list = makeOneHotEncoding(totalX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aU1IeQDx1qoC",
        "outputId": "22bb2906-6974-4ae3-e092-14e62a10e9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "!pip install scikit-multilearn\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "X = np.array(encoded_docs)\n",
        "Y = np.array(one_hot_label_list)\n",
        "classifier = BinaryRelevance(MultinomialNB())\n",
        "_ = classifier.fit(X,Y)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "atQPkjDu6Wlf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getRankedOutput2(predicted_y):\n",
        "  all_sorted_x = []\n",
        "  for i in predicted_y:\n",
        "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
        "    sorted_x = sorted(dict_temp.items(), key=lambda kv: kv[1], reverse=True) \n",
        "    all_sorted_x.append(sorted_x)\n",
        "  return all_sorted_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nxpsfKTT3SPE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def permissible(x, y):\n",
        "  if (abs(x-y)) < 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def accuracy(predicted,actual):\n",
        "    tp=0\n",
        "    length = len(actual)\n",
        "    for one_doc_idx in range(length):\n",
        "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
        "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
        "            tp+=1\n",
        "    return tp/float(length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGDpsPvsdPjJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_x_doc = readFiles(test_data = True)\n",
        "totalX_test, encoded_val_x = create_x_matrix(val_x_doc)\n",
        "ground_truth_val = makeOneHotEncoding(totalX_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "34miBk7U3Pzp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array(encoded_val_x)\n",
        "Y = np.array(ground_truth_val)\n",
        "\n",
        "val_pred = classifier.predict(X)\n",
        "val_pred_arr = val_pred.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CU7LfZKY6qFV",
        "outputId": "c6d29d72-5525-422b-8677-cbd790f0e657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validate_y_list = val_pred_arr.tolist()\n",
        "\n",
        "newv = []\n",
        "for i in range(len(validate_y_list)):\n",
        "  validate_y_list_i = list(map(int, validate_y_list[i]))\n",
        "  newv.append(validate_y_list_i)\n",
        "\n",
        "\n",
        "print (\"%f\"%accuracy(newv, ground_truth_val))\n",
        "pred_rank_list = getRankedOutput2(val_pred_arr)\n",
        "actual_rank_list = getRankedOutput2(Y)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.389130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5iNw0iiHjHQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "89398da8-59af-4077-e59b-cb50c86561dd"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', 100)\n",
        "my_df  = pd.DataFrame(columns = ['body', 'predicted ranking', 'actual ranking'])\n",
        "my_df['body'] = totalX_test\n",
        "my_df['predicted ranking'] = pred_rank_list\n",
        "my_df['actual ranking'] = actual_rank_list\n",
        "my_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>predicted ranking</th>\n",
              "      <th>actual ranking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...</td>\n",
              "      <td>[(Peoples, 14), (Topics, 2), (Places, 1), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "      <td>[(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>canadian auto workers union said accepted economic offer canadian division general motors corp g...</td>\n",
              "      <td>[(Places, 9), (Topics, 5), (Peoples, 3), (Organizations, 3), (Exchanges, 1)]</td>\n",
              "      <td>[(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...</td>\n",
              "      <td>[(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "      <td>[(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...</td>\n",
              "      <td>[(Topics, 17), (Organizations, 15), (Peoples, 7), (Places, 2), (Exchanges, 1)]</td>\n",
              "      <td>[(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...</td>\n",
              "      <td>[(Exchanges, 4), (Topics, 0), (Places, 0), (Peoples, 0), (Organizations, 0)]</td>\n",
              "      <td>[(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                  body  \\\n",
              "0  huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...   \n",
              "1  canadian auto workers union said accepted economic offer canadian division general motors corp g...   \n",
              "2  canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...   \n",
              "3  us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...   \n",
              "4  brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...   \n",
              "\n",
              "                                                                predicted ranking  \\\n",
              "0   [(Peoples, 14), (Topics, 2), (Places, 1), (Exchanges, 0), (Organizations, 0)]   \n",
              "1    [(Places, 9), (Topics, 5), (Peoples, 3), (Organizations, 3), (Exchanges, 1)]   \n",
              "2    [(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]   \n",
              "3  [(Topics, 17), (Organizations, 15), (Peoples, 7), (Places, 2), (Exchanges, 1)]   \n",
              "4    [(Exchanges, 4), (Topics, 0), (Places, 0), (Peoples, 0), (Organizations, 0)]   \n",
              "\n",
              "                                                                  actual ranking  \n",
              "0  [(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "1   [(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "2   [(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "3   [(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "4   [(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}