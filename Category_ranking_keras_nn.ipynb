{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Category_ranking_keras_nn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "4vNE_mnM0jaI",
        "outputId": "fa952e0d-65dc-4177-c1a7-3019b8793491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "#read datafolder from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder = '/content/drive/My Drive/reuters21578/'\n",
        "\n",
        "\n",
        "sgml_number_of_files = 21\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2phRP_vL0jae",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MsF3Uk8S0jaq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create category dataframe\n",
        "\n",
        "# Read all categories\n",
        "category_data = []\n",
        "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0]])\n",
        "\n",
        "# Create category dataframe\n",
        "for i in category_data:\n",
        "#     print(i[1])\n",
        "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
        "news_categories = pd.DataFrame(data=category_data)\n",
        "\n",
        "# print \"category_data: \", category_data\n",
        "#(news_categories.values).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aFMZnuLj0ja6",
        "outputId": "cf0656d3-070f-49a5-a699-6344e359f546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K-9WCTJj0jbK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def cleanUpSentence(r):#, stop_words = None#\n",
        "    r = r.lower().replace(\"<br />\", \" \")\n",
        "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
        "    r = BAD_SYMBOLS_RE.sub('', r)\n",
        "\n",
        "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
        "\n",
        "    words = word_tokenize(r)\n",
        "\n",
        "    for w in words:\n",
        "        w = lemmatizer.lemmatize(w)\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d8FTMDUo0jbX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse SGML files\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "  \n",
        "def makeDict(filename, document_X):\n",
        "  with open(filename, 'rb') as file:\n",
        "\n",
        "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
        "\n",
        "    for newsline in content('reuters'):\n",
        "      document_categories = []\n",
        "\n",
        "      document_id = newsline['newid']\n",
        "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "      if document_body == 'None':\n",
        "        continue\n",
        "\n",
        "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
        "      doc_categories = unescape(doc_categories)\n",
        "\n",
        "      document_body = unescape(document_body)\n",
        "\n",
        "      document_X[document_id] = document_body\n",
        "\n",
        "def readFiles(test_data = False):\n",
        "  document_X = {}\n",
        "  \n",
        "  if test_data == True:\n",
        "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
        "    filename = data_folder + file_name\n",
        "    makeDict(filename, document_X)\n",
        "  else:\n",
        "    for i in range(sgml_number_of_files):\n",
        "      if i < 10:\n",
        "        seq = '00' + str(i)\n",
        "      else:\n",
        "        seq = '0' + str(i)\n",
        "\n",
        "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "      print('Reading file: %s' % file_name)\n",
        "      filename = data_folder + file_name\n",
        "      makeDict(filename, document_X)\n",
        "  return document_X\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KdM_aruT0jbf",
        "outputId": "103f6e9f-c638-4e7b-a9d1-939c0762ffc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "document_X= readFiles()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-000.sgm\n",
            "Reading file: reut2-001.sgm\n",
            "Reading file: reut2-002.sgm\n",
            "Reading file: reut2-003.sgm\n",
            "Reading file: reut2-004.sgm\n",
            "Reading file: reut2-005.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-007.sgm\n",
            "Reading file: reut2-008.sgm\n",
            "Reading file: reut2-009.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-013.sgm\n",
            "Reading file: reut2-014.sgm\n",
            "Reading file: reut2-015.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-018.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-020.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "smE_sFwt0jb6",
        "outputId": "cd9e43f5-192a-40ec-811b-a5d5243a3bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def create_x_matrix(document_X):\n",
        "    totalX = []\n",
        "    for i, doc in document_X.items():\n",
        "        totalX.append(cleanUpSentence(doc))\n",
        "    max_vocab_size = 200\n",
        "    input_tokenizer = Tokenizer(200)\n",
        "    input_tokenizer.fit_on_texts(totalX)\n",
        "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
        "    return totalX,encoded_docs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pmuFLyEf0jcM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalX,encoded_docs=create_x_matrix(document_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "u9y5MsSb0jcs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create one-hot encode\n",
        "def makeOneHotEncoding(totalX):\n",
        "  words_in_body={}\n",
        "\n",
        "  for i in range(len(totalX)):\n",
        "      words=totalX[i].split(' ')\n",
        "      words_in_body[i]=words    \n",
        "\n",
        "  one_hot_label=[]\n",
        "  for key,v in words_in_body.items():\n",
        "      dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
        "      for i in v:\n",
        "          if i in category_dictionary['Topics']:\n",
        "              dict_temp['Topics']+=1\n",
        "          if i in category_dictionary['Places']:\n",
        "              dict_temp['Places']+=1\n",
        "          if i in category_dictionary['People']:\n",
        "              dict_temp['People']+=1\n",
        "          if i in category_dictionary['Exchanges']:\n",
        "              dict_temp['Exchanges']+=1\n",
        "          if i in category_dictionary['Organizations']:\n",
        "              dict_temp['Organizations']+=1\n",
        "\n",
        "      one_hot_label.append(dict_temp)\n",
        "\n",
        "\n",
        "  one_hot_label_list = []\n",
        "  for i in one_hot_label:\n",
        "\n",
        "      one_hot_label_list.append(list(i.values()))\n",
        "  return one_hot_label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5A2em3Qp5op",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_hot_label_list = makeOneHotEncoding(totalX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Au-mGTBB0jdD",
        "outputId": "1ac9a39a-5e08-4df5-faa7-58cfbf80d1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten, Dropout,Embedding\n",
        "nn = Sequential()\n",
        "max_vocab_size = 200\n",
        "nn.add(Embedding(200, 20, input_length=max_vocab_size))\n",
        "nn.add(Dense(10, activation=\"relu\", input_shape=(max_vocab_size,)))\n",
        "nn.add(Dropout(0.15))\n",
        "nn.add(Flatten())\n",
        "nn.add(Dense(5,activation=\"softmax\"))\n",
        "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "nn.fit(np.array(encoded_docs), np.array(one_hot_label_list), batch_size=16, epochs=5,\n",
        "          verbose=1, validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 14866 samples, validate on 3717 samples\n",
            "Epoch 1/5\n",
            "14866/14866 [==============================] - 4s 271us/step - loss: 3.1030 - acc: 0.8064 - val_loss: 3.1688 - val_acc: 0.7681\n",
            "Epoch 2/5\n",
            "14866/14866 [==============================] - 3s 217us/step - loss: 2.7796 - acc: 0.8147 - val_loss: 3.1070 - val_acc: 0.8036\n",
            "Epoch 3/5\n",
            "14866/14866 [==============================] - 3s 215us/step - loss: 2.7289 - acc: 0.8160 - val_loss: 3.0813 - val_acc: 0.7963\n",
            "Epoch 4/5\n",
            "14866/14866 [==============================] - 3s 217us/step - loss: 2.6849 - acc: 0.8144 - val_loss: 3.1077 - val_acc: 0.7926\n",
            "Epoch 5/5\n",
            "14866/14866 [==============================] - 3s 216us/step - loss: 2.6730 - acc: 0.8163 - val_loss: 3.1256 - val_acc: 0.8017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd30ea6bf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P5crb8zK0jdL",
        "outputId": "69da86e3-4b99-4c2e-9302-3edc83100e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_document_X = readFiles(test_data=True)\n",
        "print(len(test_document_X))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V3rAIuqZ0jdU",
        "outputId": "fdeb38c8-6524-4f0a-a917-b7f2a47a5967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "test_total_X,test_encoded_X= create_x_matrix(test_document_X)\n",
        "ground_truth_list = makeOneHotEncoding(test_total_X)\n",
        "y = nn.predict(test_encoded_X)\n",
        "print (y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.6975935  0.23532636 0.06036903 0.00425747 0.00245368]\n",
            " [0.21824951 0.5924399  0.04819081 0.0190319  0.12208779]\n",
            " [0.62695247 0.16869038 0.01963355 0.16197604 0.02274759]\n",
            " ...\n",
            " [0.6742106  0.21160336 0.05422151 0.03391149 0.02605302]\n",
            " [0.54931617 0.13644356 0.25580564 0.03680801 0.02162661]\n",
            " [0.8240941  0.11596978 0.04066919 0.00500179 0.01426519]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_P1PEjLUp5pO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def permissible(x, y):\n",
        "  if (abs(x-y)) < 2:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def accuracy(predicted,actual):\n",
        "    tp=0\n",
        "    length = len(actual)\n",
        "    for one_doc_idx in range(length):\n",
        "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
        "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
        "            tp+=1\n",
        "    return tp/float(length)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3RBUylmj0jdh",
        "outputId": "b43925cc-1a54-4e88-9e8e-aeb61f4f41ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def getRankedOutput2(predicted_y):\n",
        "    all_sorted_x = []\n",
        "    for i in predicted_y:\n",
        "        dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
        "        sorted_x = sorted(dict_temp.items(), key=lambda kv: kv[1], reverse=True) \n",
        "        all_sorted_x.append(sorted_x)\n",
        "    return all_sorted_x\n",
        "   \n",
        "print (\"%f\"%accuracy(y.tolist(), ground_truth_list))\n",
        "predicted_ranked_output = getRankedOutput2(y)\n",
        "actual_ranked_output=getRankedOutput2(np.array(ground_truth_list))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.836957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5zB_iZhbp5pi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "f4b6f249-d60a-4db4-df46-b309b533dae6"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', 100)\n",
        "my_df  = pd.DataFrame(columns = ['body', 'predicted ranking', 'actual ranking'])\n",
        "my_df['body'] = test_total_X\n",
        "my_df['predicted ranking'] = predicted_ranked_output\n",
        "my_df['actual ranking'] = actual_ranked_output\n",
        "my_df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>predicted ranking</th>\n",
              "      <th>actual ranking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...</td>\n",
              "      <td>[(Topics, 0.6975935), (Places, 0.23532636), (Peoples, 0.060369026), (Exchanges, 0.0042574685), (...</td>\n",
              "      <td>[(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>canadian auto workers union said accepted economic offer canadian division general motors corp g...</td>\n",
              "      <td>[(Places, 0.5924399), (Topics, 0.21824951), (Organizations, 0.12208779), (Peoples, 0.04819081), ...</td>\n",
              "      <td>[(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...</td>\n",
              "      <td>[(Topics, 0.62695247), (Places, 0.16869038), (Exchanges, 0.16197604), (Organizations, 0.02274759...</td>\n",
              "      <td>[(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...</td>\n",
              "      <td>[(Topics, 0.55773973), (Places, 0.22359835), (Peoples, 0.18893665), (Organizations, 0.01862973),...</td>\n",
              "      <td>[(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...</td>\n",
              "      <td>[(Topics, 0.5527812), (Exchanges, 0.26983488), (Places, 0.11818105), (Organizations, 0.029747142...</td>\n",
              "      <td>[(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                  body  \\\n",
              "0  huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...   \n",
              "1  canadian auto workers union said accepted economic offer canadian division general motors corp g...   \n",
              "2  canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...   \n",
              "3  us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...   \n",
              "4  brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...   \n",
              "\n",
              "                                                                                     predicted ranking  \\\n",
              "0  [(Topics, 0.6975935), (Places, 0.23532636), (Peoples, 0.060369026), (Exchanges, 0.0042574685), (...   \n",
              "1  [(Places, 0.5924399), (Topics, 0.21824951), (Organizations, 0.12208779), (Peoples, 0.04819081), ...   \n",
              "2  [(Topics, 0.62695247), (Places, 0.16869038), (Exchanges, 0.16197604), (Organizations, 0.02274759...   \n",
              "3  [(Topics, 0.55773973), (Places, 0.22359835), (Peoples, 0.18893665), (Organizations, 0.01862973),...   \n",
              "4  [(Topics, 0.5527812), (Exchanges, 0.26983488), (Places, 0.11818105), (Organizations, 0.029747142...   \n",
              "\n",
              "                                                                  actual ranking  \n",
              "0  [(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "1   [(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "2   [(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "3   [(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]  \n",
              "4   [(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}