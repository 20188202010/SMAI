{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2m59m68Q1qld",
    "outputId": "48eb6ef5-dee6-4278-bb07-6966354db642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#read datafolder from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLW4-GOK1ql1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vST3KK_1qmA"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "hrlSiXQl1qmP",
    "outputId": "052c3ca3-49b8-4afa-83ac-3050267f6e4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5TsBaSB1qmt"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McT2IFBv1qm3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      \n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  \n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X)\n",
    "  return document_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "xsBU6exN1qnI",
    "outputId": "b54234a5-43bd-42d9-d673-9e18625028c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "ainmgGmf1qna",
    "outputId": "d9a67431-3fc1-4b97-b1c9-10bc48ea722d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(max_vocab_size)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZPmNo0c1qnx"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM8BABxZ2p2c"
   },
   "outputs": [],
   "source": [
    "#Create one-hot encode\n",
    "def makeOneHotEncoding(totalX):\n",
    "  words_in_body={}\n",
    "\n",
    "  for i in range(len(totalX)):\n",
    "      words=totalX[i].split(' ')\n",
    "      words_in_body[i]=words    \n",
    "\n",
    "  one_hot_label=[]\n",
    "  for key,v in words_in_body.items():\n",
    "      dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "      for i in v:\n",
    "          if i in category_dictionary['Topics']:\n",
    "              dict_temp['Topics']+=1\n",
    "          if i in category_dictionary['Places']:\n",
    "              dict_temp['Places']+=1\n",
    "          if i in category_dictionary['People']:\n",
    "              dict_temp['People']+=1\n",
    "          if i in category_dictionary['Exchanges']:\n",
    "              dict_temp['Exchanges']+=1\n",
    "          if i in category_dictionary['Organizations']:\n",
    "              dict_temp['Organizations']+=1\n",
    "\n",
    "      one_hot_label.append(dict_temp)\n",
    "\n",
    "\n",
    "  one_hot_label_list = []\n",
    "  for i in one_hot_label:\n",
    "\n",
    "      one_hot_label_list.append(list(i.values()))\n",
    "  return one_hot_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcDJITG8cQ7h"
   },
   "outputs": [],
   "source": [
    "one_hot_label_list = makeOneHotEncoding(totalX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aU1IeQDx1qoC",
    "outputId": "22bb2906-6974-4ae3-e092-14e62a10e9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "!pip install scikit-multilearn\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "X = np.array(encoded_docs)\n",
    "Y = np.array(one_hot_label_list)\n",
    "classifier = BinaryRelevance(MultinomialNB())\n",
    "_ = classifier.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atQPkjDu6Wlf"
   },
   "outputs": [],
   "source": [
    "def getRankedOutput2(predicted_y):\n",
    "  all_sorted_x = []\n",
    "  for i in predicted_y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    sorted_x = sorted(dict_temp.items(), key=lambda kv: kv[1], reverse=True) \n",
    "    all_sorted_x.append(sorted_x)\n",
    "  return all_sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxpsfKTT3SPE"
   },
   "outputs": [],
   "source": [
    "def permissible(x, y):\n",
    "  if (abs(x-y)) < 2:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def accuracy(predicted,actual):\n",
    "    tp=0\n",
    "    fp = 0\n",
    "    tp_list = []\n",
    "    length = len(actual)\n",
    "    print(\"length: \",length)\n",
    "    for one_doc_idx in range(length):\n",
    "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
    "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
    "            tp+=1\n",
    "            x = tp\n",
    "        else:\n",
    "          fp = 0\n",
    "          x = fp\n",
    "        tp_list.append(x)\n",
    "    return tp/float(length), tp_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGDpsPvsdPjJ"
   },
   "outputs": [],
   "source": [
    "val_x_doc = readFiles(test_data = True)\n",
    "totalX_test, encoded_val_x = create_x_matrix(val_x_doc)\n",
    "ground_truth_val = makeOneHotEncoding(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34miBk7U3Pzp"
   },
   "outputs": [],
   "source": [
    "X = np.array(encoded_val_x)\n",
    "Y = np.array(ground_truth_val)\n",
    "\n",
    "val_pred = classifier.predict(X)\n",
    "val_pred_arr = val_pred.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CU7LfZKY6qFV",
    "outputId": "c6d29d72-5525-422b-8677-cbd790f0e657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.389130\n"
     ]
    }
   ],
   "source": [
    "validate_y_list = val_pred_arr.tolist()\n",
    "\n",
    "newv = []\n",
    "for i in range(len(validate_y_list)):\n",
    "  validate_y_list_i = list(map(int, validate_y_list[i]))\n",
    "  newv.append(validate_y_list_i)\n",
    "\n",
    "\n",
    "print (\"%f\"%accuracy(newv, ground_truth_val))\n",
    "pred_rank_list = getRankedOutput2(val_pred_arr)\n",
    "actual_rank_list = getRankedOutput2(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "5iNw0iiHjHQE",
    "outputId": "89398da8-59af-4077-e59b-cb50c86561dd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e371bf6926f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predicted ranking'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actual ranking'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted ranking'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_rank_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual ranking'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual_rank_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 100)\n",
    "my_df  = pd.DataFrame(columns = ['body', 'predicted ranking', 'actual ranking', '_'])\n",
    "my_df['body'] = totalX_test\n",
    "my_df['predicted ranking'] = pred_rank_list\n",
    "my_df['actual ranking'] = actual_rank_list\n",
    "my_df['_'] = tpl\n",
    "my_df = my_df.sort_values(by='_', ascending=False).reset_index(drop=True)\n",
    "my_df[['body', 'predicted ranking', 'actual ranking']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Binary_Relevance_Naive_Bayes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
