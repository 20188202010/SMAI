{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "# document_max_num_words = 100\n",
    "# Selected categories\n",
    "# selected_categories = ['pl_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = pd.DataFrame(data=category_data)\n",
    "\n",
    "# print \"category_data: \", category_data\n",
    "#(news_categories.values).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_category_vector(categories):\n",
    "    vector = zeros(len(categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "docid_traintest = {}\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate all files\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n",
    "# print(document_Y)\n",
    "one_hot_label=[]\n",
    "for key,v in document_Y.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'Peoples':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        string=i.split('_')\n",
    "        category=string[0]\n",
    "        if category=='to':\n",
    "            dict_temp['Topics']+=1\n",
    "        if category=='pl':\n",
    "            dict_temp['Places']+=1\n",
    "        if category=='ex':\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if category=='or':\n",
    "            dict_temp['Organizations']+=1\n",
    "        if category=='pe':\n",
    "            dict_temp['Peoples']+=1\n",
    "    one_hot_label.append(dict_temp)\n",
    "    \n",
    "# print(one_hot_label)\n",
    "ranking=[]\n",
    "for i in one_hot_label:\n",
    "    ranking.append(list(i.values()))\n",
    "#print(np.array(ranking).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showers continued throughout the week in\n",
      "the bahia cocoa zone, alleviating the drought since early\n",
      "january and improving prospects for the coming temporao,\n",
      "although normal humidity levels have not been restored,\n",
      "comissaria smith said in its weekly review.\n",
      "    the dry period means the temporao will be late this year.\n",
      "    arrivals for the week ended february 22 were 155,221 bags\n",
      "of 60 kilos making a cumulative total for the season of 5.93\n",
      "mln against 5.81 at the same stage last year. again it seems\n",
      "that cocoa delivered earlier on consignment was included in the\n",
      "arrivals figures.\n",
      "    comissaria smith said there is still some doubt as to how\n",
      "much old crop cocoa is still available as harvesting has\n",
      "practically come to an end. with total bahia crop estimates\n",
      "around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
      "are a few hundred thousand bags still in the hands of farmers,\n",
      "middlemen, exporters and processors.\n",
      "    there are doubts as to how much of this cocoa would be fit\n",
      "for export as shippers are now experiencing dificulties in\n",
      "obtaining +bahia superior+ certificates.\n",
      "    in view of the lower quality over recent weeks farmers have\n",
      "sold a good part of their cocoa held on consignment.\n",
      "    comissaria smith said spot bean prices rose to 340 to 350\n",
      "cruzados per arroba of 15 kilos.\n",
      "    bean shippers were reluctant to offer nearby shipment and\n",
      "only limited sales were booked for march shipment at 1,750 to\n",
      "1,780 dlrs per tonne to ports to be named.\n",
      "    new crop sales were also light and all to open ports with\n",
      "june/july going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
      "under new york july, aug/sept at 1,870, 1,875 and 1,880 dlrs\n",
      "per tonne fob.\n",
      "    routine sales of butter were made. march/april sold at\n",
      "4,340, 4,345 and 4,350 dlrs.\n",
      "    april/may butter went at 2.27 times new york may, june/july\n",
      "at 4,400 and 4,415 dlrs, aug/sept at 4,351 to 4,450 dlrs and at\n",
      "2.27 and 2.28 times new york sept and oct/dec at 4,480 dlrs and\n",
      "2.27 times new york dec, comissaria smith said.\n",
      "    destinations were the u.s., covertible currency areas,\n",
      "uruguay and open ports.\n",
      "    cake sales were registered at 785 to 995 dlrs for\n",
      "march/april, 785 dlrs for may, 753 dlrs for aug and 0.39 times\n",
      "new york dec for oct/dec.\n",
      "    buyers were the u.s., argentina, uruguay and convertible\n",
      "currency areas.\n",
      "    liquor sales were limited with march/april selling at 2,325\n",
      "and 2,380 dlrs, june/july at 2,375 dlrs and at 1.25 times new\n",
      "york july, aug/sept at 2,400 dlrs and at 1.25 times new york\n",
      "sept and oct/dec at 1.25 times new york dec, comissaria smith\n",
      "said.\n",
      "    total bahia sales are currently estimated at 6.13 mln bags\n",
      "against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
      "crop.\n",
      "    final figures for the period to february 28 are expected to\n",
      "be published by the brazilian cocoa trade commission after\n",
      "carnival which ends midday on february 27.\n",
      " reuter\n",
      "\u0003\n"
     ]
    }
   ],
   "source": [
    "print(document_X['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc, stop_words))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16800 samples, validate on 4200 samples\n",
      "Epoch 1/5\n",
      "16800/16800 [==============================] - 4s 218us/step - loss: 0.3125 - acc: 0.8193 - val_loss: 0.3256 - val_acc: 0.8131\n",
      "Epoch 2/5\n",
      "16800/16800 [==============================] - 3s 172us/step - loss: 0.2794 - acc: 0.8197 - val_loss: 0.3234 - val_acc: 0.8238\n",
      "Epoch 3/5\n",
      "16800/16800 [==============================] - 3s 172us/step - loss: 0.2725 - acc: 0.8204 - val_loss: 0.3256 - val_acc: 0.8165\n",
      "Epoch 4/5\n",
      "16800/16800 [==============================] - 3s 172us/step - loss: 0.2668 - acc: 0.8196 - val_loss: 0.3252 - val_acc: 0.8153\n",
      "Epoch 5/5\n",
      "16800/16800 [==============================] - 3s 173us/step - loss: 0.2618 - acc: 0.8192 - val_loss: 0.3227 - val_acc: 0.8171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d98841e80>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten, Dropout,Embedding\n",
    "nn = Sequential()\n",
    "nn.add(Embedding(1000, 20, input_length=max_vocab_size))\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(max_vocab_size,)))\n",
    "nn.add(Dropout(0.15))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(5,activation=\"softmax\"))\n",
    "# change binary_crossentropy\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(np.array(encoded_docs), np.array(ranking), batch_size=16, epochs=5,\n",
    "          verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "# Iterate all files\n",
    "document_X={}\n",
    "document_Y={}\n",
    "for i in range(21,22):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text=\"Huge oil platforms dot the Gulf like beacons -- usually lit up like Christmas trees at night. One of them, sitting astride the Rostam offshore oilfield,was all but blown out of the water by U.S. Warships on Monday.    The Iranian platform, an unsightly mass of steel andconcrete, was a three-tier structure rising 200 feet (60metres) above the warm waters of the Gulf until four U.S.Destroyers pumped some 1,000 shells into it.    The U.S. Defense Department said just 10 pct of one section of the structure remained.    U.S. helicopters destroyed three Iranian gunboats after an American helicopter came under fire earlier this month and U.S.forces attacked, seized, and sank an Iranian ship they said had been caught laying mines.    But Iran was not deterred, according to U.S. defense officials, who said Iranian forces used Chinese-made Silkworm missiles to hit a U.S.-owned Liberian-flagged ship on Thursday and the Sea Isle City on Friday.    Both ships were hit in the territorial waters of Kuwait, a key backer of Iraq in its war with Iran.    Henry Schuler, a former U.S. diplomat in the Middle Eastnow with CSIS said Washington had agreed to escort Kuwaiti tankers in order to deter Iranian attacks on shipping.    But he said the deterrence policy had failed and the level of violence and threats to shipping had increased as a result of U.S. intervention and Iran's response.    The attack on the oil platform was the latest example of a U.S. \\\"tit-for-tat\\\" policy that gave Iran the initiative, said Harlan Ullman, an ex-career naval officer now with CSIS.    He said with this appraoch America would suffer \\\"the deathof one thousand cuts.\\\"    But for the United States to grab the initiative litarily, it must take warlike steps such as mining Iran's harbors or blockading the mouth of the Gulf through which itsshipping must pass, Schuler said.    He was among those advocating mining as a means of bringing Iran to the neogtiating table. If vital supplies were cut off,Tehran could not continue the war with Iraq.    Ullman said Washington should join Moscow in a diplomatic initiative to end the war and the superpowers should impose anarms embargo against Tehran if it refused to negotiate.    He said the United States should also threaten to mine and blockade Iran if it continued fighting and must press Iraq to acknowledge responsibility for starting the war as part of asettlement.    Iranian and Western diplomats say Iraq started the war by invading Iran's territory in 1980. Iraq blames Iran for theoutbreak of hostilities, which have entailed World War I-stylenfantry attacks resulting in horrific casualties.    Each side has attacked the others' shipping.\" \n",
    "test_total_X,test_encoded_X=create_x_matrix(document_X)\n",
    "y=nn.predict(test_encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.2206326e-01 4.9578655e-01 4.5901392e-02 9.9682007e-03 2.6280599e-02]\n",
      " [8.5214394e-01 1.4697693e-01 2.2689383e-04 3.3172313e-05 6.1893353e-04]\n",
      " [4.2206326e-01 4.9578655e-01 4.5901392e-02 9.9682007e-03 2.6280599e-02]\n",
      " ...\n",
      " [4.6937662e-01 5.2064669e-01 1.3368876e-03 3.4897355e-03 5.1501370e-03]\n",
      " [6.4680630e-01 3.4982586e-01 1.3416599e-03 5.3858594e-04 1.4876238e-03]\n",
      " [5.2861172e-01 4.7051844e-01 1.7207043e-04 2.1913134e-04 4.7857504e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "# print(document_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    output.append(dict_temp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output[0])\n",
    "ranked_output=[]\n",
    "for i in output:\n",
    "    t={}\n",
    "    for key, value in sorted(i.items(), key=lambda item: item[1]):\n",
    "        t[key]=value\n",
    "    rank=0\n",
    "    for k in t.keys():\n",
    "        t[k]=rank\n",
    "        rank+=1\n",
    "    ranked_output.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ranked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
