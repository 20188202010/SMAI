{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LcfrWUr-_HnU",
    "outputId": "f3e2c207-0f84-4ffd-e60b-c7b7c966b0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#read datafolder from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSJW8QoP_XZu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0AbYqmM_bMS"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "dUnM8sNP_dec",
    "outputId": "72fb1c02-edd2-4a16-8347-9f60867c2e6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4dRTlw5_gWe"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVycH3hF_iBj"
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X, document_Y):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "\n",
    "\n",
    "\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      topics = newsline.topics.contents\n",
    "      places = newsline.places.contents\n",
    "      people = newsline.people.contents\n",
    "      orgs = newsline.orgs.contents\n",
    "      exchanges = newsline.exchanges.contents\n",
    "\n",
    "      for topic in topics:\n",
    "          document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "      for place in places:\n",
    "          document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "      for person in people:\n",
    "          document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "      for org in orgs:\n",
    "          document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "      for exchange in exchanges:\n",
    "          document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      document_Y[document_id] = document_categories\n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  document_Y = {}\n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X, document_Y)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X, document_Y)\n",
    "  return document_X, document_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "YM5RJvoX_k9R",
    "outputId": "dfffee15-e724-42ab-8513-2b6718a68eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X, document_Y = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "RBbh8HH2AQCG",
    "outputId": "c11c6f02-3f85-46ca-c21c-f8b844754ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otpy0e8PAS2x"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEv6nVfxAair"
   },
   "outputs": [],
   "source": [
    "#Create one-hot encode\n",
    "words_in_body={}\n",
    "\n",
    "for i in range(len(totalX)):\n",
    "    words=totalX[i].split(' ')\n",
    "    words_in_body[i]=words    \n",
    "\n",
    "one_hot_label=[]\n",
    "for key,v in words_in_body.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        if i in category_dictionary['Topics']:\n",
    "            dict_temp['Topics']+=1\n",
    "        if i in category_dictionary['Places']:\n",
    "            dict_temp['Places']+=1\n",
    "        if i in category_dictionary['People']:\n",
    "            dict_temp['People']+=1\n",
    "        if i in category_dictionary['Exchanges']:\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if i in category_dictionary['Organizations']:\n",
    "            dict_temp['Organizations']+=1\n",
    "            \n",
    "    one_hot_label.append(dict_temp)\n",
    "    \n",
    "\n",
    "one_hot_label_list = []\n",
    "for i in one_hot_label:\n",
    "\n",
    "    one_hot_label_list.append(list(i.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXkd6h4MAzIu"
   },
   "outputs": [],
   "source": [
    "def permissible(x, y):\n",
    "  if (abs(x-y)) < 1:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def accuracy(predicted,actual,length):\n",
    "    tp=0\n",
    "   \n",
    "    for one_doc_idx in range(length):\n",
    "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
    "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
    "            tp+=1\n",
    "    return tp/float(length)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8VvG_xMAj12"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "u2QnQFC5AmWH",
    "outputId": "1f0ae210-13fa-4e49-ea14-1c8437696829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 3 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0]\n",
      " [5 0 0 1 0]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(encoded_docs)\n",
    "y = np.array(one_hot_label_list)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Ue914-An2i"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dk_ZLGGcAtL3",
    "outputId": "014bcf3e-84d8-4ebe-eb31-c5804ee825b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999300\n"
     ]
    }
   ],
   "source": [
    "#Prediction and Calculating Accuracy\n",
    "preds = clf.predict(X)\n",
    "\n",
    "print (\"%f\"%accuracy(preds.tolist(), one_hot_label_list, len(one_hot_label_list)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Category_ranking_using_decision_tree.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
