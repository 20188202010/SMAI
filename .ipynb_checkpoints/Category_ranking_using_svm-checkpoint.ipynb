{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y8gdD8J0Itgk",
    "outputId": "c35d0fda-822b-48af-9123-ce450b2d1238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBeX9jmBItg6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAjD-9uLIthE"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
<<<<<<< HEAD
    "#     print(i[1])\n",
=======
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "IHTGtY-5IthR",
    "outputId": "8e987d7b-68f8-49e0-fb3b-9031a8707427"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAnDONjaIths"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaN3gc0HIth0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X, document_Y):\n",
<<<<<<< HEAD
    "    with open(filename, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "        document_categories = []\n",
    "        document_id = newsline['newid']\n",
    "        document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "        if document_body == 'None':\n",
    "            continue\n",
    "        doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "        doc_categories = unescape(doc_categories)\n",
    "\n",
    "        document_body = unescape(document_body)\n",
=======
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "\n",
    "\n",
    "\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    "\n",
    "      topics = newsline.topics.contents\n",
    "      places = newsline.places.contents\n",
    "      people = newsline.people.contents\n",
    "      orgs = newsline.orgs.contents\n",
    "      exchanges = newsline.exchanges.contents\n",
    "\n",
    "      for topic in topics:\n",
    "          document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "      for place in places:\n",
    "          document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "      for person in people:\n",
    "          document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "      for org in orgs:\n",
    "          document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "      for exchange in exchanges:\n",
    "          document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      document_Y[document_id] = document_categories\n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  document_Y = {}\n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X, document_Y)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X, document_Y)\n",
    "  return document_X, document_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "uqD07TIJQKy7",
    "outputId": "026274a9-5658-4bb2-cb24-53ed3223d30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X, document_Y = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qphgSScOIti3"
   },
   "outputs": [],
   "source": [
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKx4vpmHItjE"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWXPxNedJKBj"
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# # # print(document_Y)\n",
    "# one_hot_label=[]\n",
    "# dict_ranking={}\n",
    "# for key,v in document_Y.items():\n",
    "#     dict_temp={'Topics':0,'Places':0,'Peoples':0,'Exchanges':0,'Organizations':0}\n",
    "#     for i in v:\n",
    "#         string=i.split('_')\n",
    "#         category=string[0]\n",
    "#         if category=='to':\n",
    "#             dict_temp['Topics']+=1\n",
    "#         if category=='pl':\n",
    "#             dict_temp['Places']+=1\n",
    "#         if category=='ex':\n",
    "#             dict_temp['Exchanges']+=1\n",
    "#         if category=='or':\n",
    "#             dict_temp['Organizations']+=1\n",
    "#         if category=='pe':\n",
    "#             dict_temp['Peoples']+=1\n",
    "#     one_hot_label.append(dict_temp)\n",
    "    \n",
    "# #print(one_hot_label)\n",
    "# ranking=[]\n",
    "\n",
    "# for i in one_hot_label:\n",
    "#     ranking.append(list(i.values()))\n",
    "# #print(np.array(ranking).shape)\n",
=======
    "#Creating one-hot label\n",
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    "words_in_body={}\n",
    "\n",
    "for i in range(len(totalX)):\n",
    "    words=totalX[i].split(' ')\n",
    "    words_in_body[i]=words    \n",
    "\n",
    "one_hot_label=[]\n",
    "for key,v in words_in_body.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        if i in category_dictionary['Topics']:\n",
    "            dict_temp['Topics']+=1\n",
    "        if i in category_dictionary['Places']:\n",
    "            dict_temp['Places']+=1\n",
    "        if i in category_dictionary['People']:\n",
    "            dict_temp['People']+=1\n",
    "        if i in category_dictionary['Exchanges']:\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if i in category_dictionary['Organizations']:\n",
    "            dict_temp['Organizations']+=1\n",
    "            \n",
    "    one_hot_label.append(dict_temp)\n",
<<<<<<< HEAD
    "    \n",
    "\n",
    "\n",
    "# one_hot_label_list = []\n",
    "# for i in one_hot_label:\n",
    "\n",
    "#     one_hot_label_list.append(list(i.values()))"
=======
    "    "
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "Z1qFUsPkItiD",
    "outputId": "fe24c018-4dd0-49b7-e37c-bb6e7df90bd8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Topics', 7), ('Places', 3), ('People', 0), ('Exchanges', 0), ('Organizations', 0)]\n",
      "showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review dry period means temporao late year arrivals week ended february 22 155 221 bags 60 kilos making cumulative total season 593 mln 581 stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimates around 64 mln bags sales standing almost 62 mln hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would fit export shippers experiencing dificulties obtaining +bahia superior+ certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose 340 350 cruzados per arroba 15 kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment 1 750 1 780 dlrs per tonne ports named new crop sales also light open ports june july going 1 850 1 880 dlrs 35 45 dlrs new york july aug sept 1 870 1 875 1 880 dlrs per tonne fob routine sales butter made march april sold 4 340 4 345 4 350 dlrs april may butter went 227 times new york may june july 4 400 4 415 dlrs aug sept 4 351 4 450 dlrs 227 228 times new york sept oct dec 4 480 dlrs 227 times new york dec comissaria smith said destinations us covertible currency areas uruguay open ports cake sales registered 785 995 dlrs march april 785 dlrs may 753 dlrs aug 039 times new york dec oct dec buyers us argentina uruguay convertible currency areas liquor sales limited march april selling 2 325 2 380 dlrs june july 2 375 dlrs 125 times new york july aug sept 2 400 dlrs 125 times new york sept oct dec 125 times new york dec comissaria smith said total bahia sales currently estimated 613 mln bags 1986 87 crop 106 mln bags 1987 88 crop final figures period february 28 expected published brazilian cocoa trade commission carnival ends midday february 27 reuter\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#print(one_hot_label)\n",
=======
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    "import operator\n",
    "true_ranks=[]\n",
    "for i in one_hot_label:\n",
    "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(true_ranks[0])\n",
    "print(totalX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "M_GQHfFTJZpR",
    "outputId": "5e92819e-d58f-4388-fefb-3f0aab4fbb3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YIsEmSPJusK"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4vTbKfVJ1kw"
   },
   "outputs": [],
   "source": [
    "vectorizer_svm = TfidfVectorizer(stop_words='english')\n",
    "mlb = MultiLabelBinarizer()\n",
    "svmlabeltrain=[]\n",
    "svmlabeltest=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJxIV-_UJ4Cz"
   },
   "outputs": [],
   "source": [
    "for i in true_ranks[0:500]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltrain.append(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W81vWMuEJ6ig"
   },
   "outputs": [],
   "source": [
    "for i in true_ranks[500:700]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltest.append(svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JThWRX8fJ-3L",
    "outputId": "d1a75487-df7d-43de-a2e5-d8d25331068e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmlabeltrain:  <class 'list'> 500\n",
      "after fit: (500, 5)\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
<<<<<<< HEAD
    "print(\"svmlabeltrain: \",type(svmlabeltrain),len(svmlabeltrain))\n",
    "Y = mlb.fit_transform(svmlabeltrain)\n",
    "# Y_test = mlb.fit_transform(svmlabeltest)\n",
    "print(\"after fit:\",Y.shape)\n",
=======
    "Y = mlb.fit_transform(svmlabeltrain)\n",
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    "Y=np.array(Y)\n",
    "X=np.array(totalX[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "jATuWY_3KBfn",
    "outputId": "1b477efe-9e7c-4072-d445-0b9d7b3e5bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (500,)\n",
      "predicted:  (200, 5) <class 'numpy.ndarray'>\n",
      "[0 0 0 0 0]\n",
      "[('Topics', 1), ('Places', 0), ('People', 0), ('Exchanges', 0), ('Organizations', 0)]\n",
      "oper shr loss 40 cts vs loss 10 cts oper net loss 2 136 000 vs loss 467 000 revs 219 mln vs 129 mln 12 mths oper shr loss 63 cts vs loss 30 cts oper net loss 3 499 000 vs loss 1 756 000 revs 820 mln vs 545 mln note excludes income discontinued operations 1 478 000 vs 952 000 qtr 312 mln vs 6 500 000 year excludes extraordinary charge 2 503 000 current qtr 4 744 000 year reuter\n"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "classifier.fit(X, Y)\n",
<<<<<<< HEAD
    "print(\"X:\",X.shape)\n",
    "X_test=np.array(totalX[500:700])\n",
    "# print(totalX[600])\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "# print(\"X_test:\",X_test.shape)\n",
    "print(\"predicted: \",predicted.shape, type(predicted))\n",
    "print(predicted[0])\n",
    "print(true_ranks[500])\n",
    "print(totalX[500])"
=======
    "X_test=np.array(totalX[500:700])\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "# print(predicted[0])\n",
    "# print(true_ranks[500])\n",
    "# print(totalX[500])"
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QPYTWp7DItjX",
    "outputId": "9d9662de-1770-4106-c304-43eda618aa9c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# print(predicted)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print(all_labels[0])\n",
    "# print(len(svmlabeltest))"
=======
    "all_labels = mlb.inverse_transform(predicted)\n",
    "# print(all_labels[0])"
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Category_ranking_using_svm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.7"
=======
   "version": "3.7.0"
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
