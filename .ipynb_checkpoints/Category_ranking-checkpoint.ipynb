{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "# document_max_num_words = 100\n",
    "# Selected categories\n",
    "# selected_categories = ['pl_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = pd.DataFrame(data=category_data)\n",
    "\n",
    "# print \"category_data: \", category_data\n",
    "#(news_categories.values).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_category_vector(categories):\n",
    "    vector = zeros(len(categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "docid_traintest = {}\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate all files\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n",
    "# print(document_Y)\n",
    "one_hot_label=[]\n",
    "for key,v in document_Y.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'Peoples':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        string=i.split('_')\n",
    "        category=string[0]\n",
    "        if category=='to':\n",
    "            dict_temp['Topics']+=1\n",
    "        if category=='pl':\n",
    "            dict_temp['Places']+=1\n",
    "        if category=='ex':\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if category=='or':\n",
    "            dict_temp['Organizations']+=1\n",
    "        if category=='pe':\n",
    "            dict_temp['Peoples']+=1\n",
    "    one_hot_label.append(dict_temp)\n",
    "    \n",
    "# print(one_hot_label)\n",
    "ranking=[]\n",
    "for i in one_hot_label:\n",
    "    ranking.append(list(i.values()))\n",
    "#print(np.array(ranking).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_X['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc, stop_words))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten, Dropout,Embedding\n",
    "nn = Sequential()\n",
    "nn.add(Embedding(200, 20, input_length=max_vocab_size))\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(max_vocab_size,)))\n",
    "nn.add(Dropout(0.15))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(5,activation=\"softmax\"))\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(np.array(encoded_docs), np.array(ranking), batch_size=16, epochs=5,\n",
    "          verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate all files\n",
    "document_X={}\n",
    "document_Y={}\n",
    "for i in range(21,22):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text=\"Huge oil platforms dot the Gulf like beacons -- usually lit up like Christmas trees at night. One of them, sitting astride the Rostam offshore oilfield,was all but blown out of the water by U.S. Warships on Monday.    The Iranian platform, an unsightly mass of steel andconcrete, was a three-tier structure rising 200 feet (60metres) above the warm waters of the Gulf until four U.S.Destroyers pumped some 1,000 shells into it.    The U.S. Defense Department said just 10 pct of one section of the structure remained.    U.S. helicopters destroyed three Iranian gunboats after an American helicopter came under fire earlier this month and U.S.forces attacked, seized, and sank an Iranian ship they said had been caught laying mines.    But Iran was not deterred, according to U.S. defense officials, who said Iranian forces used Chinese-made Silkworm missiles to hit a U.S.-owned Liberian-flagged ship on Thursday and the Sea Isle City on Friday.    Both ships were hit in the territorial waters of Kuwait, a key backer of Iraq in its war with Iran.    Henry Schuler, a former U.S. diplomat in the Middle Eastnow with CSIS said Washington had agreed to escort Kuwaiti tankers in order to deter Iranian attacks on shipping.    But he said the deterrence policy had failed and the level of violence and threats to shipping had increased as a result of U.S. intervention and Iran's response.    The attack on the oil platform was the latest example of a U.S. \\\"tit-for-tat\\\" policy that gave Iran the initiative, said Harlan Ullman, an ex-career naval officer now with CSIS.    He said with this appraoch America would suffer \\\"the deathof one thousand cuts.\\\"    But for the United States to grab the initiative litarily, it must take warlike steps such as mining Iran's harbors or blockading the mouth of the Gulf through which itsshipping must pass, Schuler said.    He was among those advocating mining as a means of bringing Iran to the neogtiating table. If vital supplies were cut off,Tehran could not continue the war with Iraq.    Ullman said Washington should join Moscow in a diplomatic initiative to end the war and the superpowers should impose anarms embargo against Tehran if it refused to negotiate.    He said the United States should also threaten to mine and blockade Iran if it continued fighting and must press Iraq to acknowledge responsibility for starting the war as part of asettlement.    Iranian and Western diplomats say Iraq started the war by invading Iran's territory in 1980. Iraq blames Iran for theoutbreak of hostilities, which have entailed World War I-stylenfantry attacks resulting in horrific casualties.    Each side has attacked the others' shipping.\" \n",
    "test_total_X,test_encoded_X=create_x_matrix(document_X)\n",
    "y=nn.predict(test_encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "# print(document_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    output.append(dict_temp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output[0])\n",
    "ranked_output=[]\n",
    "for i in output:\n",
    "    t={}\n",
    "    for key, value in sorted(i.items(), key=lambda item: item[1]):\n",
    "        t[key]=value\n",
    "    rank=0\n",
    "    for k in t.keys():\n",
    "        t[k]=rank\n",
    "        rank+=1\n",
    "    ranked_output.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
