{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y8gdD8J0Itgk",
    "outputId": "c35d0fda-822b-48af-9123-ce450b2d1238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
<<<<<<< HEAD
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "y8gdD8J0Itgk",
        "outputId": "99fed94c-4036-45ca-d128-50429294134b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
        "data_folder = '/content/drive/My Drive/reuters21578/'\n",
        "\n",
        "sgml_number_of_files = 21\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NBeX9jmBItg6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UAjD-9uLIthE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read all categories\n",
        "category_data = []\n",
        "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0]])\n",
        "\n",
        "# Create category dataframe\n",
        "for i in category_data:\n",
        "#     print(i[1])\n",
        "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
        "news_categories = pd.DataFrame(data=category_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IHTGtY-5IthR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "15bc5c14-c1ef-420d-dd36-8ed7cd381e0e"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
=======
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBeX9jmBItg6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAjD-9uLIthE"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "IHTGtY-5IthR",
    "outputId": "8e987d7b-68f8-49e0-fb3b-9031a8707427"
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "IAnDONjaIths",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def cleanUpSentence(r):#, stop_words = None#\n",
        "    r = r.lower().replace(\"<br />\", \" \")\n",
        "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
        "    r = BAD_SYMBOLS_RE.sub('', r)\n",
        "\n",
        "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
        "\n",
        "    words = word_tokenize(r)\n",
        "\n",
        "    for w in words:\n",
        "        w = lemmatizer.lemmatize(w)\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jaN3gc0HIth0",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse SGML files\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "  \n",
        "def makeDict(filename, document_X, document_Y):\n",
        "    with open(filename, 'rb') as file:\n",
        "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
        "\n",
        "    for newsline in content('reuters'):\n",
        "        document_categories = []\n",
        "        document_id = newsline['newid']\n",
        "        document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "        if document_body == 'None':\n",
        "            continue\n",
        "        doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
        "        doc_categories = unescape(doc_categories)\n",
        "\n",
        "        document_body = unescape(document_body)\n",
        "        topics = newsline.topics.contents\n",
        "        places = newsline.places.contents\n",
        "        people = newsline.people.contents\n",
        "        orgs = newsline.orgs.contents\n",
        "        exchanges = newsline.exchanges.contents\n",
        "        for topic in topics:\n",
        "            document_categories.append('to_' + strip_tags(str(topic)))\n",
        "\n",
        "        for place in places:\n",
        "            document_categories.append('pl_' + strip_tags(str(place)))\n",
        "        for person in people:\n",
        "            document_categories.append('pe_' + strip_tags(str(person)))\n",
        "        for org in orgs:\n",
        "            document_categories.append('or_' + strip_tags(str(org)))\n",
        "        for exchange in exchanges:\n",
        "            document_categories.append('ex_' + strip_tags(str(exchange)))\n",
        "        document_X[document_id] = document_body\n",
        "        document_Y[document_id] = document_categories\n",
        "\n",
        "def readFiles(test_data = False):\n",
        "    document_X = {}\n",
        "    document_Y = {}\n",
        "    if test_data == True:\n",
        "        file_name = sgml_file_name_template.replace('NNN', '021')\n",
        "        filename = data_folder + file_name\n",
        "        makeDict(filename, document_X, document_Y)\n",
        "    else:\n",
        "        for i in range(sgml_number_of_files):\n",
        "            if i < 10:\n",
        "                seq = '00' + str(i)\n",
        "            else:\n",
        "                seq = '0' + str(i)\n",
        "        file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "        print('Reading file: %s' % file_name)\n",
        "        filename = data_folder + file_name\n",
        "        makeDict(filename, document_X, document_Y)\n",
        "    return document_X, document_Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uqD07TIJQKy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cc95f7e-f4e5-43ee-cb6b-1321b42cfa98"
      },
      "cell_type": "code",
      "source": [
        "document_X, document_Y = readFiles()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-020.sgm\n"
          ],
          "name": "stdout"
        }
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAnDONjaIths"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaN3gc0HIth0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X, document_Y):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "\n",
    "\n",
    "\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      topics = newsline.topics.contents\n",
    "      places = newsline.places.contents\n",
    "      people = newsline.people.contents\n",
    "      orgs = newsline.orgs.contents\n",
    "      exchanges = newsline.exchanges.contents\n",
    "\n",
    "      for topic in topics:\n",
    "          document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "      for place in places:\n",
    "          document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "      for person in people:\n",
    "          document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "      for org in orgs:\n",
    "          document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "      for exchange in exchanges:\n",
    "          document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      document_Y[document_id] = document_categories\n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  document_Y = {}\n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X, document_Y)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X, document_Y)\n",
    "  return document_X, document_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "uqD07TIJQKy7",
    "outputId": "026274a9-5658-4bb2-cb24-53ed3223d30e"
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "qphgSScOIti3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5e0405ee-1aa4-4d77-e8cf-426501b188fc"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "def create_x_matrix(document_X):\n",
        "    totalX = []\n",
        "    for i, doc in document_X.items():\n",
        "        totalX.append(cleanUpSentence(doc))\n",
        "    max_vocab_size = 200\n",
        "    input_tokenizer = Tokenizer(200)\n",
        "    input_tokenizer.fit_on_texts(totalX)\n",
        "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
        "    return totalX,encoded_docs"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rKx4vpmHItjE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalX,encoded_docs=create_x_matrix(document_X)"
      ],
      "execution_count": 0,
      "outputs": []
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X, document_Y = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qphgSScOIti3"
   },
   "outputs": [],
   "source": [
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKx4vpmHItjE"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWXPxNedJKBj"
   },
   "outputs": [],
   "source": [
    "#Creating one-hot label\n",
    "words_in_body={}\n",
    "\n",
    "for i in range(len(totalX)):\n",
    "    words=totalX[i].split(' ')\n",
    "    words_in_body[i]=words    \n",
    "\n",
    "one_hot_label=[]\n",
    "for key,v in words_in_body.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        if i in category_dictionary['Topics']:\n",
    "            dict_temp['Topics']+=1\n",
    "        if i in category_dictionary['Places']:\n",
    "            dict_temp['Places']+=1\n",
    "        if i in category_dictionary['People']:\n",
    "            dict_temp['People']+=1\n",
    "        if i in category_dictionary['Exchanges']:\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if i in category_dictionary['Organizations']:\n",
    "            dict_temp['Organizations']+=1\n",
    "            \n",
    "    one_hot_label.append(dict_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "Z1qFUsPkItiD",
    "outputId": "fe24c018-4dd0-49b7-e37c-bb6e7df90bd8",
    "scrolled": true
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "zWXPxNedJKBj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "words_in_body={}\n",
        "\n",
        "for i in range(len(totalX)):\n",
        "    words=totalX[i].split(' ')\n",
        "    words_in_body[i]=words    \n",
        "\n",
        "one_hot_label=[]\n",
        "for key,v in words_in_body.items():\n",
        "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
        "    for i in v:\n",
        "        if i in category_dictionary['Topics']:\n",
        "            dict_temp['Topics']+=1\n",
        "        if i in category_dictionary['Places']:\n",
        "            dict_temp['Places']+=1\n",
        "        if i in category_dictionary['People']:\n",
        "            dict_temp['People']+=1\n",
        "        if i in category_dictionary['Exchanges']:\n",
        "            dict_temp['Exchanges']+=1\n",
        "        if i in category_dictionary['Organizations']:\n",
        "            dict_temp['Organizations']+=1\n",
        "            \n",
        "    one_hot_label.append(dict_temp)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Topics', 7), ('Places', 3), ('People', 0), ('Exchanges', 0), ('Organizations', 0)]\n",
      "showers continued throughout week bahia cocoa zone alleviating drought since early january improving prospects coming temporao although normal humidity levels restored comissaria smith said weekly review dry period means temporao late year arrivals week ended february 22 155 221 bags 60 kilos making cumulative total season 593 mln 581 stage last year seems cocoa delivered earlier consignment included arrivals figures comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimates around 64 mln bags sales standing almost 62 mln hundred thousand bags still hands farmers middlemen exporters processors doubts much cocoa would fit export shippers experiencing dificulties obtaining +bahia superior+ certificates view lower quality recent weeks farmers sold good part cocoa held consignment comissaria smith said spot bean prices rose 340 350 cruzados per arroba 15 kilos bean shippers reluctant offer nearby shipment limited sales booked march shipment 1 750 1 780 dlrs per tonne ports named new crop sales also light open ports june july going 1 850 1 880 dlrs 35 45 dlrs new york july aug sept 1 870 1 875 1 880 dlrs per tonne fob routine sales butter made march april sold 4 340 4 345 4 350 dlrs april may butter went 227 times new york may june july 4 400 4 415 dlrs aug sept 4 351 4 450 dlrs 227 228 times new york sept oct dec 4 480 dlrs 227 times new york dec comissaria smith said destinations us covertible currency areas uruguay open ports cake sales registered 785 995 dlrs march april 785 dlrs may 753 dlrs aug 039 times new york dec oct dec buyers us argentina uruguay convertible currency areas liquor sales limited march april selling 2 325 2 380 dlrs june july 2 375 dlrs 125 times new york july aug sept 2 400 dlrs 125 times new york sept oct dec 125 times new york dec comissaria smith said total bahia sales currently estimated 613 mln bags 1986 87 crop 106 mln bags 1987 88 crop final figures period february 28 expected published brazilian cocoa trade commission carnival ends midday february 27 reuter\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "true_ranks=[]\n",
    "for i in one_hot_label:\n",
    "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(true_ranks[0])\n",
    "print(totalX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "M_GQHfFTJZpR",
    "outputId": "5e92819e-d58f-4388-fefb-3f0aab4fbb3a"
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "Z1qFUsPkItiD",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84179218-4389-4958-d405-e7ed7e3f1ea0"
      },
      "cell_type": "code",
      "source": [
        "#print(one_hot_label)\n",
        "import operator\n",
        "true_ranks=[]\n",
        "for i in one_hot_label:\n",
        "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
        "print(true_ranks[0])\n",
        "#print(totalX[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Topics', 7), ('People', 3), ('Places', 1), ('Exchanges', 0), ('Organizations', 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M_GQHfFTJZpR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YIsEmSPJusK"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4vTbKfVJ1kw"
   },
   "outputs": [],
   "source": [
    "vectorizer_svm = TfidfVectorizer(stop_words='english')\n",
    "mlb = MultiLabelBinarizer()\n",
    "svmlabeltrain=[]\n",
    "svmlabeltest=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJxIV-_UJ4Cz"
   },
   "outputs": [],
   "source": [
    "for i in true_ranks[0:500]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltrain.append(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W81vWMuEJ6ig"
   },
   "outputs": [],
   "source": [
    "for i in true_ranks[500:700]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltest.append(svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "JThWRX8fJ-3L",
    "outputId": "d1a75487-df7d-43de-a2e5-d8d25331068e"
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "7YIsEmSPJusK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f4vTbKfVJ1kw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer_svm = TfidfVectorizer(stop_words='english')\n",
        "mlb = MultiLabelBinarizer()\n",
        "svmlabeltrain=[]\n",
        "svmlabeltest=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DJxIV-_UJ4Cz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa41289c-3534-4b30-93e2-de55fffaa314"
      },
      "cell_type": "code",
      "source": [
        "for i in true_ranks[0:500]:\n",
        "    svm=[]\n",
        "    for j in i:\n",
        "        if j[1]!=0:\n",
        "            svm.append(j[0])\n",
        "    svmlabeltrain.append(svm)\n",
        "print(svmlabeltrain[0])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Topics', 'People', 'Places']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W81vWMuEJ6ig",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in true_ranks[500:700]:\n",
        "    svm=[]\n",
        "    for j in i:\n",
        "        if j[1]!=0:\n",
        "            svm.append(j[0])\n",
        "    svmlabeltest.append(svm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JThWRX8fJ-3L",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "#print(\"svmlabeltrain: \",type(svmlabeltrain),len(svmlabeltrain))\n",
        "Y = mlb.fit_transform(svmlabeltrain)\n",
        "\n",
        "Y=np.array(Y)\n",
        "X=np.array(totalX[0:500])"
      ],
      "execution_count": 0,
      "outputs": []
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmlabeltrain:  <class 'list'> 500\n",
      "after fit: (500, 5)\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(svmlabeltrain)\n",
    "Y=np.array(Y)\n",
    "X=np.array(totalX[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "jATuWY_3KBfn",
    "outputId": "1b477efe-9e7c-4072-d445-0b9d7b3e5bf1"
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "colab_type": "code",
        "id": "jATuWY_3KBfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e861478c-f45a-41f0-e443-6c731cc508cf"
      },
      "cell_type": "code",
      "source": [
        "classifier = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
        "#print(Y[0])\n",
        "classifier.fit(X, Y)\n",
        "print(\"X:\",X.shape)\n",
        "X_test=np.array(totalX[500:700])\n",
        "print(totalX[600])\n",
        "\n",
        "predicted = classifier.predict(X_test)\n",
        "# print(\"X_test:\",X_test.shape)\n",
        "print(\"predicted: \",predicted.shape, type(predicted))\n",
        "#print(predicted)\n",
        "print(true_ranks[0])\n",
        "#print(totalX[500])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (500,)\n",
            "shr 83 cts vs 67 cts net 10 052 000 vs 7 929 000 avg shrs 12 161 000 vs 11 824 000 nine mths shr 233 dlrs vs 191 dlrs net 28 110 000 vs 22 386 000 avg shrs 12 078 000 vs 11 729 000 assets 59 billion vs 57 billion deposits 53 billion vs 48 billion loans leases 42 billion vs 40 billion reuter\n",
            "predicted:  (200, 5) <class 'numpy.ndarray'>\n",
            "[('Topics', 7), ('People', 3), ('Places', 1), ('Exchanges', 0), ('Organizations', 0)]\n"
          ],
          "name": "stdout"
        }
      ]
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (500,)\n",
      "predicted:  (200, 5) <class 'numpy.ndarray'>\n",
      "[0 0 0 0 0]\n",
      "[('Topics', 1), ('Places', 0), ('People', 0), ('Exchanges', 0), ('Organizations', 0)]\n",
      "oper shr loss 40 cts vs loss 10 cts oper net loss 2 136 000 vs loss 467 000 revs 219 mln vs 129 mln 12 mths oper shr loss 63 cts vs loss 30 cts oper net loss 3 499 000 vs loss 1 756 000 revs 820 mln vs 545 mln note excludes income discontinued operations 1 478 000 vs 952 000 qtr 312 mln vs 6 500 000 year excludes extraordinary charge 2 503 000 current qtr 4 744 000 year reuter\n"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "classifier.fit(X, Y)\n",
    "X_test=np.array(totalX[500:700])\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "# print(predicted[0])\n",
    "# print(true_ranks[500])\n",
    "# print(totalX[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    },
    "colab_type": "code",
    "id": "QPYTWp7DItjX",
    "outputId": "9d9662de-1770-4106-c304-43eda618aa9c",
    "scrolled": true
   },
   "outputs": [
    {
<<<<<<< HEAD
      "metadata": {
        "id": "ANupfo5BoKZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "1ed984b7-f732-4ef8-836d-e3e2bd3fb6be"
      },
      "cell_type": "code",
      "source": [
        "# from sklearn.multiclass import OneVsRestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "# tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,2), token_pattern='(\\S+)')\n",
        "# X_train = tfidf_vectorizer.fit_transform(totalX[0:500])\n",
        "#    # X_val = tfidf_vectorizer.transform(X_val)\n",
        "# X_test = tfidf_vectorizer.transform(totalX[500:700])\n",
        "# y_train = mlb.fit_transform(svmlabeltrain)\n",
        "# clf = LogisticRegression(C=5,solver='liblinear')\n",
        "#     #clf = OneVsRestClassifier(clf)\n",
        "# clf.fit(X_train, y_train)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-0f7ac94d3ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#clf = OneVsRestClassifier(clf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (500, 5)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7mW11acezHy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QPYTWp7DItjX",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d09a4e0-07a0-48b0-a77c-50e47f9588cb"
      },
      "cell_type": "code",
      "source": [
        "# print(predicted)\n",
        "all_labels = mlb.inverse_transform(predicted)\n",
        "print(all_labels[0])\n",
        "# print(len(svmlabeltest))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n"
          ],
          "name": "stdout"
        }
      ]
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
>>>>>>> f95a1773e61a21a634a82c3f4c002a384551c7e4
    }
   ],
   "source": [
    "all_labels = mlb.inverse_transform(predicted)\n",
    "# print(all_labels[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Category_ranking_using_svm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
