{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Category_ranking_using_decision_tree.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "LcfrWUr-_HnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3e2c207-0f84-4ffd-e60b-c7b7c966b0d2"
      },
      "cell_type": "code",
      "source": [
        "#read datafolder from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
        "\n",
        "\n",
        "sgml_number_of_files = 21\n",
        "sgml_file_name_template = 'reut2-NNN.sgm'\n",
        "\n",
        "# Category files\n",
        "category_files = {\n",
        "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
        "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
        "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
        "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
        "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jSJW8QoP_XZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0AbYqmM_bMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create category dataframe\n",
        "\n",
        "# Read all categories\n",
        "category_data = []\n",
        "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
        "for category_prefix in category_files.keys():\n",
        "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
        "        for category in file.readlines():\n",
        "            category_data.append([category_prefix + category.strip().lower(), \n",
        "                                  category_files[category_prefix][0]])\n",
        "\n",
        "# Create category dataframe\n",
        "for i in category_data:\n",
        "#     print(i[1])\n",
        "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
        "news_categories = pd.DataFrame(data=category_data)\n",
        "\n",
        "# print \"category_data: \", category_data\n",
        "#(news_categories.values).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dUnM8sNP_dec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "72fb1c02-edd2-4a16-8347-9f60867c2e6f"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.sax.saxutils as saxutils\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "a4dRTlw5_gWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def cleanUpSentence(r):#, stop_words = None#\n",
        "    r = r.lower().replace(\"<br />\", \" \")\n",
        "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
        "    r = BAD_SYMBOLS_RE.sub('', r)\n",
        "\n",
        "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
        "\n",
        "    words = word_tokenize(r)\n",
        "\n",
        "    for w in words:\n",
        "        w = lemmatizer.lemmatize(w)\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVycH3hF_iBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse SGML files\n",
        "def strip_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def unescape(text):\n",
        "    return saxutils.unescape(text)\n",
        "  \n",
        "def makeDict(filename, document_X, document_Y):\n",
        "  with open(filename, 'rb') as file:\n",
        "\n",
        "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
        "\n",
        "    for newsline in content('reuters'):\n",
        "      document_categories = []\n",
        "\n",
        "      document_id = newsline['newid']\n",
        "\n",
        "\n",
        "\n",
        "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
        "      if document_body == 'None':\n",
        "        continue\n",
        "\n",
        "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
        "      doc_categories = unescape(doc_categories)\n",
        "\n",
        "      document_body = unescape(document_body)\n",
        "\n",
        "      topics = newsline.topics.contents\n",
        "      places = newsline.places.contents\n",
        "      people = newsline.people.contents\n",
        "      orgs = newsline.orgs.contents\n",
        "      exchanges = newsline.exchanges.contents\n",
        "\n",
        "      for topic in topics:\n",
        "          document_categories.append('to_' + strip_tags(str(topic)))\n",
        "\n",
        "      for place in places:\n",
        "          document_categories.append('pl_' + strip_tags(str(place)))\n",
        "\n",
        "      for person in people:\n",
        "          document_categories.append('pe_' + strip_tags(str(person)))\n",
        "\n",
        "      for org in orgs:\n",
        "          document_categories.append('or_' + strip_tags(str(org)))\n",
        "\n",
        "      for exchange in exchanges:\n",
        "          document_categories.append('ex_' + strip_tags(str(exchange)))\n",
        "\n",
        "      document_X[document_id] = document_body\n",
        "      document_Y[document_id] = document_categories\n",
        "\n",
        "def readFiles(test_data = False):\n",
        "  document_X = {}\n",
        "  document_Y = {}\n",
        "  if test_data == True:\n",
        "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
        "    filename = data_folder + file_name\n",
        "    makeDict(filename, document_X, document_Y)\n",
        "  else:\n",
        "    for i in range(sgml_number_of_files):\n",
        "      if i < 10:\n",
        "        seq = '00' + str(i)\n",
        "      else:\n",
        "        seq = '0' + str(i)\n",
        "\n",
        "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
        "      print('Reading file: %s' % file_name)\n",
        "      filename = data_folder + file_name\n",
        "      makeDict(filename, document_X, document_Y)\n",
        "  return document_X, document_Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YM5RJvoX_k9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "dfffee15-e724-42ab-8513-2b6718a68eb6"
      },
      "cell_type": "code",
      "source": [
        "document_X, document_Y = readFiles()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-000.sgm\n",
            "Reading file: reut2-001.sgm\n",
            "Reading file: reut2-002.sgm\n",
            "Reading file: reut2-003.sgm\n",
            "Reading file: reut2-004.sgm\n",
            "Reading file: reut2-005.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-007.sgm\n",
            "Reading file: reut2-008.sgm\n",
            "Reading file: reut2-009.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-013.sgm\n",
            "Reading file: reut2-014.sgm\n",
            "Reading file: reut2-015.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-018.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-020.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RBbh8HH2AQCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c11c6f02-3f85-46ca-c21c-f8b844754ce8"
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def create_x_matrix(document_X):\n",
        "    totalX = []\n",
        "    for i, doc in document_X.items():\n",
        "        totalX.append(cleanUpSentence(doc))\n",
        "    max_vocab_size = 200\n",
        "    input_tokenizer = Tokenizer(200)\n",
        "    input_tokenizer.fit_on_texts(totalX)\n",
        "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
        "    return totalX,encoded_docs"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Otpy0e8PAS2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalX,encoded_docs=create_x_matrix(document_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BEv6nVfxAair",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_in_body={}\n",
        "\n",
        "for i in range(len(totalX)):\n",
        "    words=totalX[i].split(' ')\n",
        "    words_in_body[i]=words    \n",
        "\n",
        "one_hot_label=[]\n",
        "for key,v in words_in_body.items():\n",
        "    dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
        "    for i in v:\n",
        "        if i in category_dictionary['Topics']:\n",
        "            dict_temp['Topics']+=1\n",
        "        if i in category_dictionary['Places']:\n",
        "            dict_temp['Places']+=1\n",
        "        if i in category_dictionary['People']:\n",
        "            dict_temp['People']+=1\n",
        "        if i in category_dictionary['Exchanges']:\n",
        "            dict_temp['Exchanges']+=1\n",
        "        if i in category_dictionary['Organizations']:\n",
        "            dict_temp['Organizations']+=1\n",
        "            \n",
        "    one_hot_label.append(dict_temp)\n",
        "    \n",
        "\n",
        "one_hot_label_list = []\n",
        "for i in one_hot_label:\n",
        "\n",
        "    one_hot_label_list.append(list(i.values()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXkd6h4MAzIu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def permissible(x, y):\n",
        "  if (abs(x-y)) < 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def accuracy(predicted,actual,length):\n",
        "    tp=0\n",
        "   \n",
        "    for one_doc_idx in range(length):\n",
        "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
        "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
        "            tp+=1\n",
        "    return tp/float(length)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8VvG_xMAj12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2QnQFC5AmWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1f0ae210-13fa-4e49-ea14-1c8437696829"
      },
      "cell_type": "code",
      "source": [
        "X = np.array(encoded_docs)\n",
        "y = np.array(one_hot_label_list)\n",
        "print (y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7 3 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " ...\n",
            " [0 0 0 0 0]\n",
            " [5 0 0 1 0]\n",
            " [1 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b7Ue914-An2i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(random_state=0).fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dk_ZLGGcAtL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "014bcf3e-84d8-4ebe-eb31-c5804ee825b0"
      },
      "cell_type": "code",
      "source": [
        "preds = clf.predict(X)\n",
        "# print (preds.tolist())\n",
        "\n",
        "print (\"%f\"%accuracy(preds.tolist(), one_hot_label_list, len(one_hot_label_list)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.999300\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}