{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "# document_max_num_words = 100\n",
    "# Selected categories\n",
    "# selected_categories = ['pl_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_acq</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to_alum</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to_austdlr</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to_austral</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to_barley</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to_bfr</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to_bop</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>to_can</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>to_carcass</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to_castor-meal</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to_castor-oil</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to_castorseed</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to_citruspulp</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to_cocoa</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to_coconut</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>to_coconut-oil</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>to_coffee</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>to_copper</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>to_copra-cake</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>to_corn</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to_corn-oil</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>to_cornglutenfeed</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to_cotton</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>to_cotton-meal</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>to_cotton-oil</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>to_cottonseed</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to_cpi</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to_cpu</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>to_crude</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>to_cruzado</td>\n",
       "      <td>Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>ex_cse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>ex_fox</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>ex_fse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>ex_hkse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>ex_ipe</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>ex_jse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>ex_klce</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>ex_klse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>ex_liffe</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>ex_lme</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>ex_lse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>ex_mase</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>ex_mise</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>ex_mnse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>ex_mose</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>ex_nasdaq</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>ex_nyce</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ex_nycsce</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>ex_nymex</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>ex_nyse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>ex_ose</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>ex_pse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>ex_set</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>ex_simex</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>ex_sse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>ex_stse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>ex_tose</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>ex_tse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>ex_wce</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>ex_zse</td>\n",
       "      <td>Exchanges</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0          1\n",
       "0               to_acq     Topics\n",
       "1              to_alum     Topics\n",
       "2           to_austdlr     Topics\n",
       "3           to_austral     Topics\n",
       "4            to_barley     Topics\n",
       "5               to_bfr     Topics\n",
       "6               to_bop     Topics\n",
       "7               to_can     Topics\n",
       "8           to_carcass     Topics\n",
       "9       to_castor-meal     Topics\n",
       "10       to_castor-oil     Topics\n",
       "11       to_castorseed     Topics\n",
       "12       to_citruspulp     Topics\n",
       "13            to_cocoa     Topics\n",
       "14          to_coconut     Topics\n",
       "15      to_coconut-oil     Topics\n",
       "16           to_coffee     Topics\n",
       "17           to_copper     Topics\n",
       "18       to_copra-cake     Topics\n",
       "19             to_corn     Topics\n",
       "20         to_corn-oil     Topics\n",
       "21   to_cornglutenfeed     Topics\n",
       "22           to_cotton     Topics\n",
       "23      to_cotton-meal     Topics\n",
       "24       to_cotton-oil     Topics\n",
       "25       to_cottonseed     Topics\n",
       "26              to_cpi     Topics\n",
       "27              to_cpu     Topics\n",
       "28            to_crude     Topics\n",
       "29          to_cruzado     Topics\n",
       "..                 ...        ...\n",
       "642             ex_cse  Exchanges\n",
       "643             ex_fox  Exchanges\n",
       "644             ex_fse  Exchanges\n",
       "645            ex_hkse  Exchanges\n",
       "646             ex_ipe  Exchanges\n",
       "647             ex_jse  Exchanges\n",
       "648            ex_klce  Exchanges\n",
       "649            ex_klse  Exchanges\n",
       "650           ex_liffe  Exchanges\n",
       "651             ex_lme  Exchanges\n",
       "652             ex_lse  Exchanges\n",
       "653            ex_mase  Exchanges\n",
       "654            ex_mise  Exchanges\n",
       "655            ex_mnse  Exchanges\n",
       "656            ex_mose  Exchanges\n",
       "657          ex_nasdaq  Exchanges\n",
       "658            ex_nyce  Exchanges\n",
       "659          ex_nycsce  Exchanges\n",
       "660           ex_nymex  Exchanges\n",
       "661            ex_nyse  Exchanges\n",
       "662             ex_ose  Exchanges\n",
       "663             ex_pse  Exchanges\n",
       "664             ex_set  Exchanges\n",
       "665           ex_simex  Exchanges\n",
       "666             ex_sse  Exchanges\n",
       "667            ex_stse  Exchanges\n",
       "668            ex_tose  Exchanges\n",
       "669             ex_tse  Exchanges\n",
       "670             ex_wce  Exchanges\n",
       "671             ex_zse  Exchanges\n",
       "\n",
       "[672 rows x 2 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = pd.DataFrame(data=category_data)\n",
    "\n",
    "# print \"category_data: \", category_data\n",
    "news_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_category_vector(categories):\n",
    "    vector = zeros(len(categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "docid_traintest = {}\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate all files\n",
    "# for i in range(sgml_number_of_files):\n",
    "#     if i < 10:\n",
    "#         seq = '00' + str(i)\n",
    "#     else:\n",
    "#         seq = '0' + str(i)\n",
    "        \n",
    "#     file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "#     print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "with open(data_folder+'reut2-000.sgm', 'rb') as file:\n",
    "    \n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "        document_categories = []\n",
    "\n",
    "        # News-line Id\n",
    "        document_id = newsline['newid']\n",
    "#             print document_id,\n",
    "        train_test = newsline['lewissplit']\n",
    "        docid_traintest[document_id] = train_test\n",
    "#             print \"train_test: \",train_test\n",
    "\n",
    "        # News-line text\n",
    "        document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "        doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "        doc_categories = unescape(doc_categories)\n",
    "\n",
    "        document_body = unescape(document_body)\n",
    "\n",
    "        # News-line categories\n",
    "        topics = newsline.topics.contents\n",
    "        places = newsline.places.contents\n",
    "        people = newsline.people.contents\n",
    "        orgs = newsline.orgs.contents\n",
    "        exchanges = newsline.exchanges.contents\n",
    "\n",
    "        for topic in topics:\n",
    "            document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "        for place in places:\n",
    "            document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "        for person in people:\n",
    "            document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "        for org in orgs:\n",
    "            document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "        for exchange in exchanges:\n",
    "            document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "#             print \"document_categories: \",document_categories\n",
    "        # Create new document    \n",
    "#             update_frequencies(document_categories)\n",
    "\n",
    "        document_X[document_id] = document_body\n",
    "        document_Y[document_id] = document_categories\n",
    "#print(document_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shower continued throughout week inthe bahia cocoa zone alleviating drought since earlyjanuary improving prospect coming temporaoalthough normal humidity level restoredcomissaria smith said weekly review dry period mean temporao late year arrival week ended february 22 155221 bagsof 60 kilo making cumulative total season 593mln 581 stage last year seemsthat cocoa delivered earlier consignment wa included thearrivals figure comissaria smith said still doubt howmuch old crop cocoa still available harvesting haspractically come end total bahia crop estimatesaround 64 mln bag sale standing almost 62 mln thereare hundred thousand bag still hand farmersmiddlemen exporter processor doubt much cocoa would fitfor export shipper experiencing dificulties inobtaining bahia superior certificate view lower quality recent week farmer havesold good part cocoa held consignment comissaria smith said spot bean price rose 340 350cruzados per arroba 15 kilo bean shipper reluctant offer nearby shipment andonly limited sale booked march shipment 1750 to1780 dlrs per tonne port named new crop sale also light open port withjunejuly going 1850 1880 dlrs 35 45 dlrsunder new york july augsept 1870 1875 1880 dlrsper tonne fob routine sale butter made marchapril sold at4340 4345 4350 dlrs aprilmay butter went 227 time new york may junejulyat 4400 4415 dlrs augsept 4351 4450 dlrs at227 228 time new york sept octdec 4480 dlrs and227 time new york dec comissaria smith said destination u covertible currency areasuruguay open port cake sale registered 785 995 dlrs formarchapril 785 dlrs may 753 dlrs aug 039 timesnew york dec octdec buyer u argentina uruguay convertiblecurrency area liquor sale limited marchapril selling 2325and 2380 dlrs junejuly 2375 dlrs 125 time newyork july augsept 2400 dlrs 125 time new yorksept octdec 125 time new york dec comissaria smithsaid total bahia sale currently estimated 613 mln bagsagainst 198687 crop 106 mln bag 198788crop final figure period february 28 expected tobe published brazilian cocoa trade commission aftercarnival end midday february 27 reuter\n"
     ]
    }
   ],
   "source": [
    "totalX = []\n",
    "#totalY = np.array(document_Y)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "#print(document_X.keys())\n",
    "for i, doc in document_X.items():\n",
    "    #print(i)\n",
    "    totalX.append(cleanUpSentence(doc, stop_words))\n",
    "\n",
    "\n",
    "print(totalX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200)\n",
      "input_vocab_size: 17601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_vocab_size = 200\n",
    "input_tokenizer = Tokenizer(200)# change accuracy....\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "#print(input_tokenizer.word_counts)\n",
    "encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "print(encoded_docs.shape)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX)))\n",
    "# print(input_tokenizer.word_counts)\n",
    "# print(t.document_count)\n",
    "# print(t.word_index)\n",
    "# print(t.word_docs)\n",
    "# print(totalX.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-7e4faeee30f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmultilabel_binarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmultilabel_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_questions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilabel_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_questions' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_questions.Tags)\n",
    "y = multilabel_binarizer.classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
