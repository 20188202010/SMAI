{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getkey(x):\n",
    "    if x=='to_':\n",
    "        return 'Topics'\n",
    "    elif x=='pl_':\n",
    "        return 'Places'\n",
    "    elif x=='pe_':\n",
    "        return 'Peoples'\n",
    "    elif x=='or_':\n",
    "        return 'Organizations'\n",
    "    elif x=='ex_':\n",
    "        return 'Exchanges'\n",
    "    \n",
    "category_data = []\n",
    "knn_cats={}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        k=getkey(category_prefix)\n",
    "        knn_cats[k]=file.readlines()\n",
    "        knn_cats[k]=[s.rstrip('\\n') for s in knn_cats[k]]\n",
    "        strcat=\" \"\n",
    "        knn_cats[k]=[strcat.join(knn_cats[k])]\n",
    "        for category in file.readlines():\n",
    "            #knn_cats[category_prefix].append(category.strip().lower())\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = pd.DataFrame(data=category_data)\n",
    "#print(knn_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_category_vector(categories):\n",
    "    vector = zeros(len(categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "docid_traintest = {}\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate all files\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n",
    "# print(document_Y)\n",
    "one_hot_label=[]\n",
    "dict_ranking={}\n",
    "for key,v in document_Y.items():\n",
    "    dict_temp={'Topics':0,'Places':0,'Peoples':0,'Exchanges':0,'Organizations':0}\n",
    "    for i in v:\n",
    "        string=i.split('_')\n",
    "        category=string[0]\n",
    "        if category=='to':\n",
    "            dict_temp['Topics']+=1\n",
    "        if category=='pl':\n",
    "            dict_temp['Places']+=1\n",
    "        if category=='ex':\n",
    "            dict_temp['Exchanges']+=1\n",
    "        if category=='or':\n",
    "            dict_temp['Organizations']+=1\n",
    "        if category=='pe':\n",
    "            dict_temp['Peoples']+=1\n",
    "    one_hot_label.append(dict_temp)\n",
    "    \n",
    "#print(one_hot_label)\n",
    "ranking=[]\n",
    "\n",
    "for i in one_hot_label:\n",
    "    ranking.append(list(i.values()))\n",
    "#print(np.array(ranking).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Places', 3), ('Topics', 1), ('Peoples', 0), ('Exchanges', 0), ('Organizations', 0)]\n"
     ]
    }
   ],
   "source": [
    "#print(one_hot_label)\n",
    "import operator\n",
    "true_ranks=[]\n",
    "for i in one_hot_label:\n",
    "    true_ranks.append(sorted(i.items(),key=operator.itemgetter(1),reverse=True))\n",
    "print(true_ranks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(document_Y['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(document_X['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sarvat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc, stop_words))\n",
    "    max_vocab_size = 200\n",
    "    input_tokenizer = Tokenizer(200)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "Accuracy KNN  0.6376\n"
     ]
    }
   ],
   "source": [
    "def accuracy_knn(predicted,actual,length):\n",
    "    tp=0\n",
    "    for i in range(length):\n",
    "        #print(predicted[i][0][0],actual[i][0][0])\n",
    "        if predicted[i][0][0]==actual[i][0][0] or predicted[i][1][0]==actual[i][1][0]: #or predicted[i][2][0]==actual[i][2][0] :\n",
    "            tp+=1\n",
    "    return tp/length\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "#print(len(totalX))\n",
    "# tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set) \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "resultsknn=[]\n",
    "for i in totalX[0:10000]:\n",
    "    sim={}\n",
    "    for k,j in knn_cats.items():\n",
    "        #print(i)\n",
    "        sim[k]=(cosine_sim(i,j[0]))\n",
    "    resultsknn.append(sorted(sim.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    #print(sorted(sim.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print(\"results\")\n",
    "#testknn=\n",
    "#print(resultsknn)\n",
    "accuracy=accuracy_knn(resultsknn,true_ranks,10000)\n",
    "print(\"Accuracy KNN \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmlabeltrain: (500, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected indicator for 4 classes, but got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cd78e4e9f73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# print(predicted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m#print(all_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, yt)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             raise ValueError('Expected indicator for {0} classes, but got {1}'\n\u001b[0;32m--> 959\u001b[0;31m                              .format(len(self.classes_), yt.shape[1]))\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected indicator for 4 classes, but got 5"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "vectorizer_svm = TfidfVectorizer(stop_words='english',\n",
    "                             )\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "svmlabeltrain=[]\n",
    "svmlabeltest=[]\n",
    "\n",
    "for i in true_ranks[0:500]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltrain.append(svm)\n",
    "for i in true_ranks[500:700]:\n",
    "    svm=[]\n",
    "    for j in i:\n",
    "        if j[1]!=0:\n",
    "            svm.append(j[0])\n",
    "    svmlabeltest.append(svm)\n",
    "    \n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(svmlabeltrain)\n",
    "Y_test = mlb.fit_transform(svmlabeltest)\n",
    "print(\"svmlabeltrain:\",Y.shape)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "Y=np.array(Y)\n",
    "X=np.array(totalX[0:500])\n",
    "#print(Y.shape)\n",
    "classifier.fit(X, Y)\n",
    "X_test=np.array(totalX[500:700])\n",
    "predicted = classifier.predict(X_test)\n",
    "# print(predicted)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "#print(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X_train = np.array([\"new york is a hell of a town\",\n",
    "                    \"new york was originally dutch\",\n",
    "                    \"the big apple is great\",\n",
    "                    \"new york is also called the big apple\",\n",
    "                    \"nyc is nice\",\n",
    "                    \"people abbreviate new york city as nyc\",\n",
    "                    \"the capital of great britain is london\",\n",
    "                    \"london is in the uk\",\n",
    "                    \"london is in england\",\n",
    "                    \"london is in great britain\",\n",
    "                    \"it rains a lot in london\",\n",
    "                    \"london hosts the british museum\",\n",
    "                    \"new york is great and so is london\",\n",
    "                    \"i like london better than new york\"])\n",
    "y_train_text = [[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],\n",
    "                [\"new york\"],[\"london\"],[\"london\"],[\"london\"],[\"london\"],\n",
    "                [\"london\"],[\"london\"],[\"new york\",\"london\"],[\"new york\",\"london\"]]\n",
    "\n",
    "X_test = np.array(['nice day in nyc',\n",
    "                   'welcome to london',\n",
    "                   'london is rainy',\n",
    "                   'it is raining in britian',\n",
    "                   'it is raining in britian and the big apple',\n",
    "                   'it is raining in britian and nyc',\n",
    "                   'hello welcome to new york. enjoy it here and london too'])\n",
    "target_names = ['New York', 'London']\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train_text)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y)\n",
    "print(Y.shape,X_train.shape)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(predicted)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "for item, labels in zip(X_test, all_labels):\n",
    "    print('{0} => {1}'.format(item, ', '.join(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten, Dropout,Embedding\n",
    "nn = Sequential()\n",
    "nn.add(Embedding(200, 20, input_length=max_vocab_size))\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(max_vocab_size,)))\n",
    "nn.add(Dropout(0.15))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(5,activation=\"softmax\"))\n",
    "nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(np.array(encoded_docs), np.array(ranking), batch_size=16, epochs=5,\n",
    "          verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate all files\n",
    "document_X={}\n",
    "document_Y={}\n",
    "for i in range(21,22):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    #data_folder + file_name\n",
    "    with open(data_folder+file_name, 'rb') as file:\n",
    "        content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "\n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "        #             print document_id,\n",
    "            train_test = newsline['lewissplit']\n",
    "            docid_traintest[document_id] = train_test\n",
    "        #             print \"train_test: \",train_test\n",
    "\n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            doc_categories=strip_tags(str(newsline('topics')[0].body))\n",
    "            doc_categories = unescape(doc_categories)\n",
    "\n",
    "            document_body = unescape(document_body)\n",
    "\n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "\n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "\n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "\n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "\n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "\n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "        #             print \"document_categories: \",document_categories\n",
    "            # Create new document    \n",
    "        #             update_frequencies(document_categories)\n",
    "\n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = document_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text=\"Huge oil platforms dot the Gulf like beacons -- usually lit up like Christmas trees at night. One of them, sitting astride the Rostam offshore oilfield,was all but blown out of the water by U.S. Warships on Monday.    The Iranian platform, an unsightly mass of steel andconcrete, was a three-tier structure rising 200 feet (60metres) above the warm waters of the Gulf until four U.S.Destroyers pumped some 1,000 shells into it.    The U.S. Defense Department said just 10 pct of one section of the structure remained.    U.S. helicopters destroyed three Iranian gunboats after an American helicopter came under fire earlier this month and U.S.forces attacked, seized, and sank an Iranian ship they said had been caught laying mines.    But Iran was not deterred, according to U.S. defense officials, who said Iranian forces used Chinese-made Silkworm missiles to hit a U.S.-owned Liberian-flagged ship on Thursday and the Sea Isle City on Friday.    Both ships were hit in the territorial waters of Kuwait, a key backer of Iraq in its war with Iran.    Henry Schuler, a former U.S. diplomat in the Middle Eastnow with CSIS said Washington had agreed to escort Kuwaiti tankers in order to deter Iranian attacks on shipping.    But he said the deterrence policy had failed and the level of violence and threats to shipping had increased as a result of U.S. intervention and Iran's response.    The attack on the oil platform was the latest example of a U.S. \\\"tit-for-tat\\\" policy that gave Iran the initiative, said Harlan Ullman, an ex-career naval officer now with CSIS.    He said with this appraoch America would suffer \\\"the deathof one thousand cuts.\\\"    But for the United States to grab the initiative litarily, it must take warlike steps such as mining Iran's harbors or blockading the mouth of the Gulf through which itsshipping must pass, Schuler said.    He was among those advocating mining as a means of bringing Iran to the neogtiating table. If vital supplies were cut off,Tehran could not continue the war with Iraq.    Ullman said Washington should join Moscow in a diplomatic initiative to end the war and the superpowers should impose anarms embargo against Tehran if it refused to negotiate.    He said the United States should also threaten to mine and blockade Iran if it continued fighting and must press Iraq to acknowledge responsibility for starting the war as part of asettlement.    Iranian and Western diplomats say Iraq started the war by invading Iran's territory in 1980. Iraq blames Iran for theoutbreak of hostilities, which have entailed World War I-stylenfantry attacks resulting in horrific casualties.    Each side has attacked the others' shipping.\" \n",
    "test_total_X,test_encoded_X=create_x_matrix(document_X)\n",
    "y=nn.predict(test_encoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "# print(document_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    output.append(dict_temp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output[0])\n",
    "ranked_output=[]\n",
    "for i in output:\n",
    "    t={}\n",
    "    for key, value in sorted(i.items(), key=lambda item: item[1]):\n",
    "        t[key]=value\n",
    "    rank=0\n",
    "    for k in t.keys():\n",
    "        t[k]=rank\n",
    "        rank+=1\n",
    "    ranked_output.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
