Why Decision Tree
coz Algorithm adaptation
- C4.5 is the traditional decision tree algorithm
	- builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy.
- MultiLabel Decision Tree: Extension of the popular C4.5 decision tree algorithm; with multi-label entropy

- to support multi-output problems, following changes are done:
	- Store n output values in leaves, instead of 1;
	
Advantages:
- Simple to understand and to interpret. Trees can be visualised.
- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
- In general, run time cost to construct a balanced binary tree is O(nsamples nfeatures log⁡(nsamples)) and query time O(log⁡(nsamples)). 

- problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. 
- Decision tree learners create biased trees if some classes dominate.

Improvements
to avoid creating over - complex tree
- pruning should be done
-  setting the minimum number of samples required at a leaf node
- ensemble learning mitigates the problem of unstable nature of decision tree

