{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LcfrWUr-_HnU",
    "outputId": "9fb487ae-7075-4b07-d57e-c7f146b87935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#read datafolder from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_folder = '/content/drive/My Drive/pg2k18/sem2/smai/smai_proj/reuters21578/'\n",
    "\n",
    "\n",
    "sgml_number_of_files = 21\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSJW8QoP_XZu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0AbYqmM_bMS"
   },
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "category_dictionary={'Topics':[],'Places':[],'People':[],'Organizations':[],'Exchanges':[]}\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0]])\n",
    "\n",
    "# Create category dataframe\n",
    "for i in category_data:\n",
    "    category_dictionary[i[1]].append(i[0].split('_')[1])\n",
    "news_categories = pd.DataFrame(data=category_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "dUnM8sNP_dec",
    "outputId": "804fe4e6-8da2-482b-a874-b67290a4d9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4dRTlw5_gWe"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def cleanUpSentence(r):#, stop_words = None#\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = REPLACE_BY_SPACE_RE.sub(' ', r)\n",
    "    r = BAD_SYMBOLS_RE.sub('', r)\n",
    "\n",
    "    r = ' '.join(word for word in r.split() if word not in STOPWORDS)\n",
    "\n",
    "    words = word_tokenize(r)\n",
    "\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVycH3hF_iBj"
   },
   "outputs": [],
   "source": [
    "# Parse SGML files\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "  \n",
    "def makeDict(filename, document_X):\n",
    "  with open(filename, 'rb') as file:\n",
    "\n",
    "    content = BeautifulSoup(file.read().lower(),'html.parser')\n",
    "\n",
    "    for newsline in content('reuters'):\n",
    "      document_categories = []\n",
    "\n",
    "      document_id = newsline['newid']\n",
    "      document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "      if document_body == 'None':\n",
    "        continue\n",
    "\n",
    "      doc_categories = strip_tags(str(newsline('topics')[0].body))\n",
    "      doc_categories = unescape(doc_categories)\n",
    "\n",
    "      document_body = unescape(document_body)\n",
    "\n",
    "      document_X[document_id] = document_body\n",
    "      \n",
    "\n",
    "def readFiles(test_data = False):\n",
    "  document_X = {}\n",
    "  \n",
    "  if test_data == True:\n",
    "    file_name = sgml_file_name_template.replace('NNN', '021')\n",
    "    filename = data_folder + file_name\n",
    "    makeDict(filename, document_X)\n",
    "  else:\n",
    "    for i in range(sgml_number_of_files):\n",
    "      if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "      else:\n",
    "        seq = '0' + str(i)\n",
    "\n",
    "      file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "      print('Reading file: %s' % file_name)\n",
    "      filename = data_folder + file_name\n",
    "      makeDict(filename, document_X)\n",
    "  return document_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "YM5RJvoX_k9R",
    "outputId": "9fa643ff-314e-4d5c-8c4d-e628c74b7a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X = readFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8XmkzW2ng93z",
    "outputId": "14e1d262-6cc9-486d-93e2-2ec0adf3b013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18583\n"
     ]
    }
   ],
   "source": [
    "print(len(document_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "n5tlfxGMZM5o",
    "outputId": "19166ace-e456-4253-a243-697cd0ff2c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBbh8HH2AQCG"
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 200\n",
    "def create_x_matrix(document_X):\n",
    "    totalX = []\n",
    "    for i, doc in document_X.items():\n",
    "        totalX.append(cleanUpSentence(doc))\n",
    "    \n",
    "    input_tokenizer = Tokenizer(max_vocab_size)\n",
    "    input_tokenizer.fit_on_texts(totalX)\n",
    "    encoded_docs = input_tokenizer.texts_to_matrix(totalX, mode='count')\n",
    "    return totalX,encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otpy0e8PAS2x"
   },
   "outputs": [],
   "source": [
    "totalX,encoded_docs=create_x_matrix(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEv6nVfxAair"
   },
   "outputs": [],
   "source": [
    "#Create one-hot encode\n",
    "def makeOneHotEncoding(totalX):\n",
    "  words_in_body={}\n",
    "\n",
    "  for i in range(len(totalX)):\n",
    "      words=totalX[i].split(' ')\n",
    "      words_in_body[i]=words    \n",
    "\n",
    "  one_hot_label=[]\n",
    "  for key,v in words_in_body.items():\n",
    "      dict_temp={'Topics':0,'Places':0,'People':0,'Exchanges':0,'Organizations':0}\n",
    "      for i in v:\n",
    "          if i in category_dictionary['Topics']:\n",
    "              dict_temp['Topics']+=1\n",
    "          if i in category_dictionary['Places']:\n",
    "              dict_temp['Places']+=1\n",
    "          if i in category_dictionary['People']:\n",
    "              dict_temp['People']+=1\n",
    "          if i in category_dictionary['Exchanges']:\n",
    "              dict_temp['Exchanges']+=1\n",
    "          if i in category_dictionary['Organizations']:\n",
    "              dict_temp['Organizations']+=1\n",
    "\n",
    "      one_hot_label.append(dict_temp)\n",
    "\n",
    "\n",
    "  one_hot_label_list = []\n",
    "  for i in one_hot_label:\n",
    "\n",
    "      one_hot_label_list.append(list(i.values()))\n",
    "  return one_hot_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GD7mTePHaSRa"
   },
   "outputs": [],
   "source": [
    "one_hot_label_list = makeOneHotEncoding(totalX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhQpFPOmfIAI"
   },
   "outputs": [],
   "source": [
    "def getRankedOutput2(predicted_y):\n",
    "  all_sorted_x = []\n",
    "  for i in predicted_y:\n",
    "    dict_temp={'Topics':i[0],'Places':i[1],'Peoples':i[2],'Exchanges':i[3],'Organizations':i[4]}\n",
    "    sorted_x = sorted(dict_temp.items(), key=lambda kv: kv[1], reverse=True) \n",
    "    all_sorted_x.append(sorted_x)\n",
    "  return all_sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXkd6h4MAzIu"
   },
   "outputs": [],
   "source": [
    "def permissible(x, y):\n",
    "  if (abs(x-y)) < 2:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def accuracy(predicted,actual):\n",
    "    tp=0\n",
    "    length = len(actual)\n",
    "    for one_doc_idx in range(length):\n",
    "        if permissible(predicted[one_doc_idx][0],actual[one_doc_idx][0]) and permissible(predicted[one_doc_idx][1],actual[one_doc_idx][1])\\\n",
    "          and permissible(predicted[one_doc_idx][2],actual[one_doc_idx][2]) and permissible(predicted[one_doc_idx][3],actual[one_doc_idx][3]):\n",
    "            tp+=1\n",
    "    return tp/float(length)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8VvG_xMAj12"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "u2QnQFC5AmWH",
    "outputId": "a193bfe9-71c0-45ee-8957-8c6b0e943ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 3 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0]\n",
      " [5 0 0 1 0]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(encoded_docs)\n",
    "y = np.array(one_hot_label_list)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Ue914-An2i"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFz3j8DUZhou"
   },
   "outputs": [],
   "source": [
    "doc_x_test = readFiles(test_data = True)\n",
    "totalX_test ,encoded_docs_test = create_x_matrix(doc_x_test)\n",
    "ground_truth_list = makeOneHotEncoding(totalX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKq13bX2aelk"
   },
   "outputs": [],
   "source": [
    "X = np.array(encoded_docs_test)\n",
    "Y = np.array(ground_truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dk_ZLGGcAtL3",
    "outputId": "27550c43-cb66-4b06-b74f-ed39d1b2d99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558696\n"
     ]
    }
   ],
   "source": [
    "#Prediction and Calculating Accuracy\n",
    "preds = clf.predict(X)\n",
    "\n",
    "print (\"%f\"%accuracy(preds.tolist(), ground_truth_list))\n",
    "pred_rank_list = getRankedOutput2(np.array(preds))\n",
    "actual_rank_list = getRankedOutput2(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "r9uGyKp8hK0F",
    "outputId": "42a12b1d-721c-48ed-9a38-c519e60e8f17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>predicted ranking</th>\n",
       "      <th>actual ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...</td>\n",
       "      <td>[(Topics, 1.0), (Places, 1.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]</td>\n",
       "      <td>[(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>canadian auto workers union said accepted economic offer canadian division general motors corp g...</td>\n",
       "      <td>[(Places, 1.0), (Topics, 0.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]</td>\n",
       "      <td>[(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...</td>\n",
       "      <td>[(Exchanges, 1.0), (Topics, 0.0), (Places, 0.0), (Peoples, 0.0), (Organizations, 0.0)]</td>\n",
       "      <td>[(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...</td>\n",
       "      <td>[(Topics, 1.0), (Places, 1.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]</td>\n",
       "      <td>[(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...</td>\n",
       "      <td>[(Topics, 2.0), (Exchanges, 2.0), (Places, 1.0), (Peoples, 0.0), (Organizations, 0.0)]</td>\n",
       "      <td>[(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  body  \\\n",
       "0  huge oil platforms dot gulf like beacons usually lit like christmas trees night one sitting astr...   \n",
       "1  canadian auto workers union said accepted economic offer canadian division general motors corp g...   \n",
       "2  canada development corp said polysar ltd unit completed refinancing package worth 830 mln canadi...   \n",
       "3  us attack iranian oil platform gulf monday appeared titfortat raid carefully orchestrated provoc...   \n",
       "4  brown disc products co inc unit fo genevar enterprises inc said purchased ongoing business trade...   \n",
       "\n",
       "                                                                        predicted ranking  \\\n",
       "0  [(Topics, 1.0), (Places, 1.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]   \n",
       "1  [(Places, 1.0), (Topics, 0.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]   \n",
       "2  [(Exchanges, 1.0), (Topics, 0.0), (Places, 0.0), (Peoples, 0.0), (Organizations, 0.0)]   \n",
       "3  [(Topics, 1.0), (Places, 1.0), (Peoples, 0.0), (Exchanges, 0.0), (Organizations, 0.0)]   \n",
       "4  [(Topics, 2.0), (Exchanges, 2.0), (Places, 1.0), (Peoples, 0.0), (Organizations, 0.0)]   \n",
       "\n",
       "                                                                  actual ranking  \n",
       "0  [(Places, 12), (Topics, 3), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
       "1   [(Places, 2), (Topics, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
       "2   [(Topics, 4), (Places, 1), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  \n",
       "3   [(Places, 6), (Peoples, 2), (Topics, 0), (Exchanges, 0), (Organizations, 0)]  \n",
       "4   [(Topics, 0), (Places, 0), (Peoples, 0), (Exchanges, 0), (Organizations, 0)]  "
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 100)\n",
    "my_df  = pd.DataFrame(columns = ['body', 'predicted ranking', 'actual ranking'])\n",
    "my_df['body'] = totalX_test\n",
    "my_df['predicted ranking'] = pred_rank_list\n",
    "my_df['actual ranking'] = actual_rank_list\n",
    "my_df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Category_ranking_using_decision_tree.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
